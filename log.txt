2018-02-27 16:34:13 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 16:34:13 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:34:16 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:34:16 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:34:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete True 332 # SA* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 False 220 False 0.6564306719064884 3 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # RobustScaler True True # SelectFwe 0.39376071555683756 chi2 # SA* SVC rbf 3 2.3839685780861314e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> | 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.072600 False 100 # SA* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # StandardScaler True True # SelectFdr 0.4608103694360143 chi2 # SA* SVC poly 3 2.4851608604406576e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SA* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:34:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:34:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:34:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA False 1198 # SA* SVC poly 3 3.535379282685604e-05 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:34:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SA* QuadraticDiscriminantAnalysis 0.6396026761675004 0.000800 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:34:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:34:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SA* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 False 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete True 173 #  SA*' RandomForestClassifier entropy True False balanced_subsample 100 True 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:25 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:34:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SA* DecisionTreeClassifier entropy best None auto 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:26 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:34:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:34:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete True 332 # SA* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 True 220 True 0.6564306719064884 3 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SA* PassiveAggressive  True 10 True hinge False None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SA* RandomForestClassifier entropy False False balanced_subsample 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SA* KNeighborsClassifier 1 uniform auto 99 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:34:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 16:34:29 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SA* KNeighborsClassifier 1 uniform auto 18 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete False 173 #  SA*' RandomForestClassifier entropy True False balanced 100 True 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 16:34:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SA* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:34:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:34:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:16 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 16:35:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:16 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-27 16:35:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-27 16:35:16 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> StandardScaler True True # FastICA deflation cube 750 0.026455 False 1 # SA*  DecisionTreeClassifier entropy best None True None 79967 0.157234 None # BernoulliNB 0.842486 6.451908 False # GaussianNB # PassiveAggressive  True 250 True hinge False balanced # RidgeClassifier 10 True sag 0.057434 8.281859 balanced True True # DecisionTreeClassifier entropy random balanced auto 0.880479 34899 0.137735 66411 # GaussianNB # RidgeCCV None 2.737897 None False False # DecisionTreeClassifier entropy random None False 0.825995 None 0.028147 1315 # MultinomialNB 3.958093 False # ExtraTreeClassifier gini best balanced False 0.757343 87022 0.156251 None # Perceptron l1 8.390315 True 750 False 0.449247 None False # MultinomialNB 5.603297 True # LinearDiscriminantAnalysis lsqr 0.035243 False # SVC sigmoid 6 0.035353 10 None # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:35:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:18 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:21 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-27 16:35:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:21 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:23 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-27 16:35:23 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:23 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:23 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 16:35:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:23 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:23 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 16:35:23 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-27 16:35:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:24 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:24 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:24 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 16:35:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:24 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:35:38 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 16:35:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFromModel 0.754517 True # SA*  PassiveAggressive  False 50 True squared_loss False None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:35:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:38 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:38 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 16:35:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:39 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:39 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> Imputer median # StandardScaler False False # SelectPercentile 35 f_classif # SA*  NuSVC sigmoid 10 0.021046 10 balanced # MultinomialNB 1.084683 True # RidgeCCV 8 8.742178 None False True # ExtraTreeClassifier entropy random None auto 0.881692 None 0.451814 58795 # SVC poly 7 0.039801 100 balanced # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:35:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:39 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-27 16:35:39 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:39 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:35:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:35:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:35:40 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:35:40 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-27 16:35:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:35:40 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:18 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-27 16:36:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:19 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-27 16:36:19 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:19 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-27 16:36:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> Imputer mean # RBFSampler 6.376265 2 # PolynomialFeatures 7 False False # SA*  RidgeCCV 7 1.979648 None True False # LogisticCV None True 300 liblinear balanced True 0.029585 # RidgeCCV None 4.939735 balanced True True # RidgeCCV None 3.990865 None True True # LogisticRegression True 450 liblinear None False 0.073269 # CentroidClassifier None chebyshev # RidgeClassifier 100 True cholesky 0.035546 2.708005 None False False # SVC linear 4 0.068751 500 balanced # RidgeClassifier 10 True cholesky 0.039927 2.896076 None False False # ExtraTreeClassifier gini best None False 0.365102 None 0.340351 None # LinearDiscriminantAnalysis lsqr 0.063926 True # MultinomialNB 3.420052 True # LogisticRegression False 100 newton-cg None False 0.057103 # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:36:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-27 16:36:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-27 16:36:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:20 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:20 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-27 16:36:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:20 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:20 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-27 16:36:20 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:20 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 16:36:21 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-27 16:36:21 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:21 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:21 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:21 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:21 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:21 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-27 16:36:21 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-27 16:36:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:22 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-27 16:36:22 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-27 16:36:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:22 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-27 16:36:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-27 16:36:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-27 16:36:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:23 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:24 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:24 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-27 16:36:24 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-27 16:36:24 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:24 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-27 16:36:24 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:24 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-27 16:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-27 16:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-27 16:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:27 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-27 16:36:27 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-27 16:36:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:29 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-27 16:36:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:30 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:31 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-27 16:36:31 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-27 16:36:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:35 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:35 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:35 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:37 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-27 16:36:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer most_frequent # StandardScaler False False # SelectKBest 2 chi2 # PolynomialFeatures 6 True False # SA*  ExtraTreeClassifier gini best balanced False log2 None 0.429532 42679 # CentroidClassifier None euclidean # RidgeCCV None 0.360703 balanced False False # BernoulliNB 0.957988 0.195682 False # RadiusNeighborsClassifier 29.582557 uniform brute 67 15 chebyshev # MultinomialNB 0.972117 True # ExtraTreeClassifier entropy random balanced False None None 0.167480 None # DecisionTreeClassifier entropy best None auto 0.216958 None 0.339389 None # DecisionTreeClassifier gini random None False log2 None 0.196838 4581 # QuadraticDiscriminantAnalysis 0.382359 0.057912 False # NuSVC rbf 2 0.049532 100 None # MultinomialNB 3.281323 True # MultinomialNB 7.557127 False # SVC linear 6 0.087056 1000 None # BernoulliNB 0.072263 8.854745 True # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:36:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:38 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:40 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-27 16:36:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:42 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:42 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:42 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:44 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:44 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:45 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 16:36:45 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:45 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:36:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 16:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:36:49 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:50 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-27 16:36:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:36:50 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-27 16:37:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-27 16:37:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-27 16:40:14 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 16:40:22 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:40:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:40:27 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:40:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:40:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:40:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:41:18 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 16:41:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:41:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:41:21 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:41:24 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:41:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:41:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:41:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:41:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:41:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:41:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:41:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:41:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:41:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:41:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:41:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:41:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:41:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:41:44 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 False 0.5687034678818491 4 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:41:44 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:305: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.
  warn("Warm-start fitting without increasing n_estimators does not "

2018-02-27 16:41:44 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:41:44 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:43:43 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 16:43:44 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:43:44 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:43:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:43:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:43:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:43:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:43:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:43:52 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:43:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:43:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:43:53 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:43:55 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:43:55 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:43:57 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:44:02 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:44:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:44:05 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:44:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:44:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 False 0.5687034678818491 4 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:44:07 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:44:08 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:44:10 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:44:11 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:44:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 True 0.5687034678818491 4 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:44:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 False 0.5687034678818491 4 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:44:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:44:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:44:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:44:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:44:47 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 16:44:47 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:44:47 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:44:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:44:54 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:44:58 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:44:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:45:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:45:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:45:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:45:06 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:45:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:45:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:45:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:45:08 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:45:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:45:10 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:45:11 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:45:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:45:16 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:45:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:45:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:45:20 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:45:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:45:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 False 0.5687034678818491 4 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:45:22 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:45:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:45:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:46:37 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 16:46:39 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:46:39 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:46:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:46:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:46:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:46:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:46:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:46:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:46:48 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:46:48 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:46:49 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:46:49 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:46:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:46:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:46:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:46:56 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:46:56 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:46:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:46:59 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:47:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:47:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 10 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:03 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:03 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 10 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:11 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:47:11 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:11 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:47:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:20 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:305: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.
  warn("Warm-start fitting without increasing n_estimators does not "

2018-02-27 16:47:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:47:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:47:50 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 16:47:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' True 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:47:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:53 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:53 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:47:53 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:47:53 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:47:56 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:48:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:48:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' False 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:05 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:48:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:10 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:48:10 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:48:11 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 10 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:48:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 10 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:20 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:48:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:48:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:48:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:48:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 False 0.5687034678818491 4 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:26 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:48:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:29 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' True 0.4044792917812593 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:48:30 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:48:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:48:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:49:01 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 16:49:01 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:13 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 16:49:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' False 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:20 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:21 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:21 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:22 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:24 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:27 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:29 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' True 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:33 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:41 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:42 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:42 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:45 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:47 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 10 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:47 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:47 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 10 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:47 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:48 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:48 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:49 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:52 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:53 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:54 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:54 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 False 0.5368752992317617 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 False 0.5368752992317617 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 True 0.5368752992317617 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:49:58 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:49:58 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:305: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.
  warn("Warm-start fitting without increasing n_estimators does not "

2018-02-27 16:49:58 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:49:58 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:50:42 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 16:50:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:50:44 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' True 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:50:44 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' False 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:50:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:50:47 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:50:48 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:50:49 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:50:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:50:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:50:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:50:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:50:54 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:50:56 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:50:57 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:50:58 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:50:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:50:58 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:50:59 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 True 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:01 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:06 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:06 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 10 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:11 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:11 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 True 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:11 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:11 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:14 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:18 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:19 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:19 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:21 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:21 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:22 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:24 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:24 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:25 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 10 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:27 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:30 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:30 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:31 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:34 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:36 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:37 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:38 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:39 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:42 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:42 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:42 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:42 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:43 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:43 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:44 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:44 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:44 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 False 0.542144980834302 10 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:45 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:45 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:45 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:45 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:46 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:46 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:47 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:47 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:47 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:47 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:47 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:47 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:47 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:47 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:47 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:47 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:47 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:47 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:49 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:49 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:49 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:49 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' True 0.4044792917812593 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:53 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:54 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:54 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:54 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:54 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:54 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:55 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:55 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:55 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:55 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:55 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:56 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:56 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:56 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:56 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:56 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:56 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:57 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:57 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:57 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:57 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:57 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:57 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:57 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:57 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:57 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:57 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:57 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:51:58 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:58 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 False 0.542144980834302 10 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:51:58 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:58 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:58 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:58 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:59 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:59 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:59 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:59 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:59 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:59 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:59 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:51:59 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:51:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:51:59 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:00 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:00 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:00 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:01 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:01 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 True 233 True 0.542144980834302 10 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:01 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:01 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:01 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:01 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:02 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:03 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:03 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 True 0.5368752992317617 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:03 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:03 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:03 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:04 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:04 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:04 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:04 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:05 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 True 0.5368752992317617 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:06 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:06 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:06 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:06 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:52:07 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:07 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:07 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:08 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:305: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.
  warn("Warm-start fitting without increasing n_estimators does not "

2018-02-27 16:52:26 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 16:52:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA False 1198 # SAM* SVC poly 3 3.535379282685604e-05 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 True 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None True 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:52:35 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 16:52:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA False 1198 # SAM* SVC poly 3 3.535379282685604e-05 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 True 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 True 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None True 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:52:39 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None False 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:52:41 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:52:42 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:52:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:52:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' False 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean False 51 # SAM* SVC poly 3 0.010632086351533367 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 auto 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:52:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:52:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:52:53 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:55 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 auto 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:57 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:52:57 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:58 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:52:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:52:59 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:52:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:52:59 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:53:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:02 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 False 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:02 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean True 51 # SAM* SVC poly 3 0.010632086351533367 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:02 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:02 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:05 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:53:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:53:05 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:53:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:06 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:53:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 True 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:08 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:53:09 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:53:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None auto 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:11 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 False 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:53:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 True 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:13 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:53:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:53:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:53:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:53:14 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:53:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:53:14 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:53:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:53:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:53:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:53:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' True 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean False 51 # SAM* SVC poly 3 0.010632086351533367 -1 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:53:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:53:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:49 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:53:50 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:53:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:53:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:54 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:53:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:53:54 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:53:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:56 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:53:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:53:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:53:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:53:57 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:53:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:53:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:00 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:54:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:54:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:54:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 10 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:54:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:54:01 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:54:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:03 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:54:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:54:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:54:03 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:54:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:54:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:08 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:54:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:54:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:54:08 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:54:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:54:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:18 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:54:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:54:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:54:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:54:25 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:54:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:54:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:54:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:54:27 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:54:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:54:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:54:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:35 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:54:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:54:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:39 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:54:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:54:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:42 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:54:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:54:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:52 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:54:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:54:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:54:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:54:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:02 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:02 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:55:02 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:55:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 True 0.542144980834302 10 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:55:07 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:55:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:11 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:12 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 10 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:55:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None True 1..0 0.24229264852063404 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:55:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:28 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:32 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:33 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:55:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:55:34 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:55:34 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:35 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:55:35 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:55:35 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:35 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:55:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None False 1..0 0.24229264852063404 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:55:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:38 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:55:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:55:39 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:55:40 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:55:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:43 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:55:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:47 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:47 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:55:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:48 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:49 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:49 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:55:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:50 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:55:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:54 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:54 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:54 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:56 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:55:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:55:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:55:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:00 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:56:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:02 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:10 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:56:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' False 0.4044792917812593 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:56:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:21 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:21 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:56:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:25 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:56:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:27 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:29 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:56:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:30 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:56:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:40 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:56:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:47 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:47 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:56:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:48 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:56:48 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:49 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:51 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:56:51 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:55 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:57 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:57 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:56:59 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:56:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:56:59 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:56:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:00 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:57:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:57:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:57:00 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:57:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:57:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:57:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:03 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:57:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:57:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:09 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:57:09 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:57:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:10 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:57:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:57:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:57:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:24 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:57:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:57:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:57:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:25 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:57:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:57:25 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:57:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:57:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:57:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:33 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:57:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:57:33 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:57:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:57:33 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:57:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:57:33 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:57:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:57:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:57:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:38 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:57:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:57:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:57:38 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:57:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:57:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:57:44 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:57:44 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:58:25 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 16:58:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # StandardScaler True True # SelectFdr 0.4608103694360143 chi2 # SAM* SVC poly 3 2.4851608604406576e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.020445 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:58:34 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 16:58:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # StandardScaler True True # SelectFdr 0.4608103694360143 chi2 # SAM* SVC poly 3 2.4851608604406576e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.020445 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:58:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' True 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:58:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 16:58:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:58:42 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA False 1198 # SAM* SVC poly 3 3.535379282685604e-05 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:42 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None auto 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:42 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:58:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:58:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean True 51 # SAM* SVC poly 3 0.010632086351533367 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:58:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.075187 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:49 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:58:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None True 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None False 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.031736 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:58:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 auto 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:52 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:58:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:58:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:58:54 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:58:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:58:55 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' False 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.090964 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:58:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:59:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.018864 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 False 0.5687034678818491 4 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:59:01 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:59:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:59:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:59:02 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 True 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:05 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:59:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:59:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:59:05 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:59:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:06 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:59:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.090964 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:07 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:59:11 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.051671 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 False 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:13 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:59:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 auto 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:13 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:59:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:59:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 16:59:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 16:59:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.092232 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.006749 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 True 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 16:59:21 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.019822 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:21 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.008075 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 16:59:25 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:59:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 16:59:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:59:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 16:59:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:28 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:59:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 16:59:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.015590 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:30 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:59:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 16:59:31 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:59:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:33 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.055509 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:33 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 True 0.5368752992317617 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.096302 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 16:59:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.005953 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.067936 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:37 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:59:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:59:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 16:59:39 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:39 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:39 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:59:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:59:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 16:59:42 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:59:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:59:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.006282 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:46 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:59:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:59:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:59:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:48 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:59:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:59:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:59:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.040485 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.000315 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.059189 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean False 51 # SAM* SVC poly 3 0.010632086351533367 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.058251 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:51 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 16:59:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:59:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 16:59:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.087084 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.044670 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:53 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:54 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:59:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 16:59:56 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:59:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 16:59:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 16:59:58 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 16:59:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.033680 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:59 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 16:59:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:01 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:00:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:00:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:00:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:06 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:00:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:00:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.007199 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.073138 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.053430 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:10 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:00:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:00:10 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:00:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:12 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:00:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:12 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:00:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.046451 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:00:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.004669 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.049042 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:00:17 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:00:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:17 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:00:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.053126 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:21 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.091380 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None True 1..0 0.24229264852063404 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:00:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.097041 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.057754 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:26 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:00:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.061030 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 False 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:31 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.091553 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.012988 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:00:33 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.032557 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:00:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.073587 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.054536 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.020618 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 True 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.068120 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:39 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.091465 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:00:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:41 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.006255 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:41 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:41 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.010241 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:42 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:00:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:00:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.058894 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:44 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 True 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:45 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:00:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:00:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.013921 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:49 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:00:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:49 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:00:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.066211 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:54 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:00:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:00:54 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:00:54 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.009752 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:54 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 10 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:00:55 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.087772 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:56 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.045329 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.044830 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:00:59 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.073179 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:00:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.074546 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.027430 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.011302 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:02 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.079246 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:01:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.007356 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:07 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:01:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:01:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:01:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:14 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:01:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:01:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:01:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.065454 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.031203 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.071998 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:16 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:01:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:01:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:01:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:01:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.034063 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:01:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:19 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:01:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:01:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:20 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:01:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:01:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.026368 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.057902 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:01:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.014947 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.095901 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:27 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:01:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:01:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:01:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.061069 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.040713 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 False 0.542144980834302 10 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:30 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:01:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:01:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:01:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.033190 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:31 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.045407 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:01:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:36 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:01:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:01:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:01:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.084767 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:01:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.058654 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 True 0.5368752992317617 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:40 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:01:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:01:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:01:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:01:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.092358 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:01:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:53 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.078598 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:54 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:01:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:01:54 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:54 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.029361 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:01:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:57 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:01:57 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:01:57 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:01:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.015283 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.044953 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:01:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:01:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:00 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:02:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:02:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.097752 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.092510 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # StandardScaler True True # SelectFdr 0.4608103694360143 chi2 # SAM* SVC poly 3 2.4851608604406576e-05 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:01 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:02:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:02:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:02:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:04 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:02:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:02:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:02:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.044472 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:06 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:02:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:02:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:02:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 10 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.002423 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.005148 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:02:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.044554 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:02:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:02:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:02:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.037374 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:02:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 True 480 False 0.2797288369369436 10 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.058689 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:02:28 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:02:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:02:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:02:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.007587 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:30 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:02:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:02:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:02:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:02:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:35 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:02:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:02:35 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:02:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:02:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:02:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.053960 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:41 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.069229 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.044092 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.034217 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.035396 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:02:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:02:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.093352 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:49 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:02:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:02:49 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:02:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.090356 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' True 0.4044792917812593 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:49 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:02:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:02:49 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:02:49 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:02:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:02:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:02:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:02:55 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.044135 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:55 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:02:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:02:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.034146 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.072686 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:02:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:02:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:03:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.081443 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:03:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.005655 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:03:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.030198 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:03:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:03:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.099482 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:03:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:03:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:03:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.076534 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:03:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:19 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:03:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:03:19 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:03:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:03:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.073796 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:03:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.028542 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:03:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.092814 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:03:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:24 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:03:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:03:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:03:25 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:03:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:03:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.023947 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:03:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.046857 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:03:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:33 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:03:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:03:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:03:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:03:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 False 0.5687034678818491 4 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:03:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:41 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:03:41 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:03:41 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:03:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:03:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:03:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.006213 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:03:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:03:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:03:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:03:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:54 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:03:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:03:54 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.027215 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:03:54 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None auto 1..0 0.24229264852063404 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:03:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:03:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:03:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:04:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.050297 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:04:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:04:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:04:05 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:04:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:04:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:04:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.085738 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:04:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.090502 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:04:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:04:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:14 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:04:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:04:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.061523 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:04:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.092401 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:04:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.085771 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:04:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.070633 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:04:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.003989 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:04:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:21 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:04:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.067487 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:04:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.040398 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:04:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.064274 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:04:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:29 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.041476 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:04:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:04:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:04:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.044153 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:04:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.041638 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:04:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:04:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.051133 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:44 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:04:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:04:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:04:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:04:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:52 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:04:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:04:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:04:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:04:59 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.063854 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:04:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:05:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:05:01 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:05:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:05:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:05:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:05:02 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:05:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:05:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:05:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:05:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:05:55 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 17:05:55 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # RobustScaler True True # SelectFwe 0.39376071555683756 chi2 # SAM* SVC rbf 3 2.3839685780861314e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:05:55 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.095789 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:05:56 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:05:56 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.078695 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:05:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.047248 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:05:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 4 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:05:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:06:08 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 17:06:08 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:06:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # RobustScaler True True # SelectFwe 0.39376071555683756 chi2 # SAM* SVC rbf 3 2.3839685780861314e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:08 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:06:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.095789 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:06:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None False 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:10 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:06:11 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:06:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None auto 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.078695 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None True 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.047248 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:06:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:06:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean True 51 # SAM* SVC poly 3 0.010632086351533367 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # StandardScaler True True # SelectFdr 0.4608103694360143 chi2 # SAM* SVC poly 3 2.4851608604406576e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 4 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:21 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:06:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' True 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:27 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:06:27 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:06:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:06:28 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:06:29 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.034335 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:29 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.029880 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.052229 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.061669 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:32 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:06:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:06:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:06:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:06:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.010902 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.011334 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 8 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 auto 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.098041 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:06:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 False 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:41 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:44 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.014547 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:44 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 True 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:44 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 6 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:06:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:06:48 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:06:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:06:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 False 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:54 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:54 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 True 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:54 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' False 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:54 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:06:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:06:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:06:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:59 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA False 1198 # SAM* SVC poly 3 3.535379282685604e-05 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:59 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:06:59 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:06:59 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 True 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:07:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:07:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:07:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:07:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:07:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:07:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:07:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:07:11 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 2 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:07:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.010937 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # StandardScaler True True # SelectFdr 0.4608103694360143 chi2 # SAM* SVC poly 3 2.4851608604406576e-05 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.040599 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:16 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:07:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:07:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:07:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean False 51 # SAM* SVC poly 3 0.010632086351533367 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:18 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:07:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:07:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 auto 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:07:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:07:21 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:07:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:07:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:07:21 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 5 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:21 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:07:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:07:21 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:21 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.072735 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:07:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:07:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 10 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:07:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:07:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.008536 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.069854 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:07:41 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:07:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.031073 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:07:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:07:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.012173 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:07:46 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.082404 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:07:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.025163 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 True 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:52 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:07:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.070619 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.066047 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:07:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.044319 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:07:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:07:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:08:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:08:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 9 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.079958 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:08:03 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:08:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:08:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:08:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:08:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.073024 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 10 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.076001 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:14 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:08:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:08:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:08:14 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:08:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:08:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.079102 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.003503 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:16 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:08:17 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:08:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:08:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:08:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.005971 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.093419 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.018166 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:21 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.015509 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:21 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:08:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:08:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:08:21 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' True 0.4044792917812593 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.022872 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.047210 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:08:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:08:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:08:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:08:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.066706 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:31 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.018239 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:08:31 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:08:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:34 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:08:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:08:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.049377 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.004615 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:08:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:41 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.081907 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.004233 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:08:47 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:08:47 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:08:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:08:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.090286 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.038983 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:48 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:08:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:08:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.013490 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:08:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:56 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.080233 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:08:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.022377 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:08:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.002854 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.064631 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:09:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:03 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 3 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:12 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:09:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:09:12 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:09:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:09:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.062353 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.078334 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.044664 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:09:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:09:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.036224 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:21 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:09:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:09:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.062039 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.088966 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.077732 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.096037 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 True 0.542144980834302 10 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.000826 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.077956 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:09:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:29 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.008678 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.046470 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.052474 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:34 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:09:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:09:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:09:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:35 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:09:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:09:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:09:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:41 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA False 1198 # SAM* SVC poly 3 3.535379282685604e-05 -1 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:09:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:44 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:09:44 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:09:44 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.027542 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:09:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.024124 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:09:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:55 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.028082 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.085221 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.006622 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.095426 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:09:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:09:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:10:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:10:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.064133 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.025817 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:10:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.007771 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:10:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:10:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.097205 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.023885 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.089927 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:10:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:22 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:10:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:10:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.055058 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 7 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.026197 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:10:33 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.001916 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.046089 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:41 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.055399 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:10:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:47 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:10:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:10:48 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:10:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:10:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.053618 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:48 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:10:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:10:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:10:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:10:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.013360 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:10:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.088015 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:49 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:10:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:10:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.034685 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' False 0.4044792917812593 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:10:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:10:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:10:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:10:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.034915 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:11:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:11:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:11:02 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean False 51 # SAM* SVC poly 3 0.010632086351533367 -1 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:11:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.030559 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:11:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:11:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.028522 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:11:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:11:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:11 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:11:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:11:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.011406 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:11:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:11:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:11:19 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:11:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:11:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:20 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:11:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:11:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:11:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:22 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:11:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:11:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.073997 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:11:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.001166 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:11:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.020978 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:11:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.076821 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:11:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:29 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:11:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:11:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:11:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.015895 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:11:30 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:11:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:11:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:11:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.011639 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:11:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:11:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.011725 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:48 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:11:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:11:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:11:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.015234 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:11:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:12:34 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 17:12:35 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:12:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean False 51 # SAM* SVC poly 3 0.010632086351533367 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 4 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:12:36 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 17:12:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # RobustScaler True True # SelectFwe 0.39376071555683756 chi2 # SAM* SVC rbf 3 2.3839685780861314e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:12:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:12:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:37 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:12:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 44 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 False 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:12:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.035727 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:12:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> | 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.023362 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 10 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:12:41 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA False 1198 # SAM* SVC poly 3 3.535379282685604e-05 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:41 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:41 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:12:42 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None True 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:42 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:12:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:12:45 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 17:12:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:12:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:12:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # StandardScaler True True # SelectFdr 0.4608103694360143 chi2 # SAM* SVC poly 3 2.4851608604406576e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean True 51 # SAM* SVC poly 3 0.010632086351533367 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:12:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.044381 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '54.275042925685625' -> Imputer most_frequent # MinMaxScaler # SelectPercentile 54.275042925685625 f_classif # SAM* RandomForestClassifier gini True False None 100 False 0.8138233157708883 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:12:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:12:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:12:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:13:14 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 17:13:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:13:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # RobustScaler True True # SelectFwe 0.39376071555683756 chi2 # SAM* SVC rbf 3 2.3839685780861314e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:13:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.095789 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:13:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None False 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:13:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None auto 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.078695 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None True 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.047248 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:20 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:13:20 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:13:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean True 51 # SAM* SVC poly 3 0.010632086351533367 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:21 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # StandardScaler True True # SelectFdr 0.4608103694360143 chi2 # SAM* SVC poly 3 2.4851608604406576e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 4 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:13:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' True 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:13:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.034335 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.029880 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.052229 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.061669 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:13:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:13:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.010902 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.011334 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:42 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 8 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:42 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 auto 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.098041 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:13:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 False 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:46 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.014547 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 True 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 6 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:13:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:13:52 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:13:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:13:56 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 False 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 True 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' False 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:13:58 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:14:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:14:02 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:02 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA False 1198 # SAM* SVC poly 3 3.535379282685604e-05 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:03 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:14:03 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 True 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:14:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:14:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:14:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:14:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:14:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:14:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:14:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:14:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 2 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:14:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.010937 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # StandardScaler True True # SelectFdr 0.4608103694360143 chi2 # SAM* SVC poly 3 2.4851608604406576e-05 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.040599 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:14:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:14:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:14:21 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean False 51 # SAM* SVC poly 3 0.010632086351533367 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:21 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:14:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:14:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 auto 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:14:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:14:24 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:14:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:14:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:14:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 5 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:24 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:14:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:14:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:29 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.072735 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:14:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:14:31 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:33 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 10 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:14:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:14:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.008536 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:39 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.069854 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:14:44 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:14:46 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.031073 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:14:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:14:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:14:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.012173 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:49 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:14:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.082404 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:53 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:14:54 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.025163 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:14:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:15:01 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 17:15:01 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:15:02 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA False 1198 # SAM* SVC poly 3 3.535379282685604e-05 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:02 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean True 51 # SAM* SVC poly 3 0.010632086351533367 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:04 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:15:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' True 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 True 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # StandardScaler True True # SelectFdr 0.4608103694360143 chi2 # SAM* SVC poly 3 2.4851608604406576e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:15:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 10 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.006407 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 7 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.000901 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None True 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:08 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:15:09 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:15:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # RobustScaler True True # SelectFwe 0.39376071555683756 chi2 # SAM* SVC rbf 3 2.3839685780861314e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:11 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:15:11 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 True 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:11 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:15:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:15:11 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '54.275042925685625' -> Imputer most_frequent # MinMaxScaler # SelectPercentile 54.275042925685625 f_classif # SAM* RandomForestClassifier gini True False balanced 100 True 0.8138233157708883 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:15:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None auto 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:15:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:15:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.089052 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:20 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:15:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:15:20 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:15:21 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 False 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:24 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:15:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 3 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:15:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:15:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:15:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:15:28 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:15:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:15:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:15:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' False 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:15:31 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> | 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.039866 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean False 51 # SAM* SVC poly 3 0.010632086351533367 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.051160 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:305: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.
  warn("Warm-start fitting without increasing n_estimators does not "

2018-02-27 17:16:13 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 17:16:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> | 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:16:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:16:13 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:16:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 8 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # RobustScaler True True # SelectFwe 0.39376071555683756 chi2 # SAM* SVC rbf 3 2.3839685780861314e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '54.275042925685625' -> Imputer most_frequent # MinMaxScaler # SelectPercentile 54.275042925685625 f_classif # SAM* RandomForestClassifier gini True False None 100 True 0.8138233157708883 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 True 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:16:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA False 1198 # SAM* SVC poly 3 3.535379282685604e-05 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None auto 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:16:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:16:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:16:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' False 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' True 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # StandardScaler True True # SelectFdr 0.4608103694360143 chi2 # SAM* SVC poly 3 2.4851608604406576e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None False 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.062329 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '54.275042925685625' -> Imputer most_frequent # MinMaxScaler # SelectPercentile 54.275042925685625 f_classif # SAM* RandomForestClassifier gini True False None 100 False 0.8138233157708883 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean False 51 # SAM* SVC poly 3 0.010632086351533367 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:16:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 4 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None True 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:16:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:16:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean True 51 # SAM* SVC poly 3 0.010632086351533367 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 True 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:16:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 5 0.00090919103756734 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:305: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.
  warn("Warm-start fitting without increasing n_estimators does not "

2018-02-27 17:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:17:01 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 17:17:01 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:17:02 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # RobustScaler True True # SelectFwe 0.39376071555683756 chi2 # SAM* SVC rbf 3 2.3839685780861314e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:02 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:17:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.095789 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:17:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None False 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:04 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:17:04 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:17:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None auto 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.078695 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None True 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.047248 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:07 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:17:08 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:17:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean True 51 # SAM* SVC poly 3 0.010632086351533367 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # StandardScaler True True # SelectFdr 0.4608103694360143 chi2 # SAM* SVC poly 3 2.4851608604406576e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 4 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:17:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' True 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:20 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:17:20 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:17:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:17:20 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:17:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.034335 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.029880 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.052229 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.061669 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:25 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:17:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:17:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:17:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:17:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.010902 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.011334 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:29 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 8 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 4 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 auto 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.098041 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:30 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:17:33 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 False 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:33 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.014547 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 True 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 6 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:17:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:17:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:17:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:17:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 False 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 True 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:46 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' False 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:17:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:17:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:17:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA False 1198 # SAM* SVC poly 3 3.535379282685604e-05 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:17:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 True 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:17:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:17:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:17:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:17:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:17:56 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:17:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:17:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:17:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:17:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:17:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:18:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:18:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:18:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:18:03 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 2 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:18:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.010937 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False balanced_subsample 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # StandardScaler True True # SelectFdr 0.4608103694360143 chi2 # SAM* SVC poly 3 2.4851608604406576e-05 -1 balanced
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.040599 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:07 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:18:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:18:09 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:18:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean False 51 # SAM* SVC poly 3 0.010632086351533367 -1 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:10 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:18:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:18:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 auto 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:18:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:18:12 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:18:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:18:12 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:18:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 5 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:13 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:18:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:18:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 True 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 True 408 False 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.072735 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:18:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:18:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 10 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:18:25 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:18:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.008536 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 7 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.069854 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:31 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 True 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:18:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '2.8382117756438676' -> Imputer mean # SelectPercentile 2.8382117756438676 f_classif # SAM* RandomForestClassifier entropy False False None 100 False 0.5368752992317617 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:18:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.031073 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:18:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:18:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.012173 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:18:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:39 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.082404 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:18:42 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.025163 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 True 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:18:44 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.070619 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:44 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.066047 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:44 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 9 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:18:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.044319 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:18:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:18:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 9 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.079958 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:18:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:18:55 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:18:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:18:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:18:55 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:18:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:18:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.073024 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 10 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.076001 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:05 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:19:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:19:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:19:05 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:19:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:19:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.079102 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.003503 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:07 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:19:08 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:19:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:19:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:19:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.005971 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.093419 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.018166 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.015509 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:12 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:19:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:19:12 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:19:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' True 0.4044792917812593 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.022872 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.047210 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:19:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:19:16 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:19:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:19:16 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:19:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:19:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:19:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 auto 233 False 0.542144980834302 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 auto 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:19:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:19:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.066706 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:21 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.018239 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:21 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:19:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:19:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 5 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:19:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:24 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:19:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:19:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.049377 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:29 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.004615 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:19:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.081907 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:33 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.004233 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:19:38 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:19:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:19:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:19:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.090286 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.038983 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:19:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:19:42 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.013490 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:19:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:46 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.080233 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.022377 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.002854 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.064631 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:19:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:53 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 2 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:19:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:19:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 3 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:02 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 6 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:02 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:20:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:20:02 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:20:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:20:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.062353 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.078334 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.044664 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:20:06 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:20:07 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:20:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:20:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.036224 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:12 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:20:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:20:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.062039 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.088966 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.077732 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:20:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.096037 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: 'NoneType' object has no attribute 'predict' -> Imputer mean # RobustScaler True True # SelectFwe 0.0614425536709615 f_classif # SAM* GradientBoostingClassifier' deviance 0.053517066400173056 0.7398539900055563 False 233 True 0.542144980834302 10 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.000826 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.077956 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:20:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.008678 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.046470 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.052474 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:25 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:20:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:20:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:20:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:20:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:20:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:20:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA False 1198 # SAM* SVC poly 3 3.535379282685604e-05 -1 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:20:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:34 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:20:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:20:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.027542 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:20:42 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.024124 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:20:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.028082 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:47 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.085221 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:47 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.006622 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.095426 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:20:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:20:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:54 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.064133 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:54 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.025817 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:20:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:20:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:20:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.007771 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:21:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:21:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.097205 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.023885 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.089927 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:21:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:21:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:21:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.055058 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1754 must be between 0 and n_features=8 with svd_solver='full' -> Imputer mean # Normalizer l1 # TraditionalPCA True 1754 # SAM* SVC rbf 7 0.00090919103756734 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.026197 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:21:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.001916 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.046089 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:31 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.055399 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:21:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:38 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:21:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:21:38 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:21:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:21:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.053618 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:39 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:21:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:21:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:21:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:21:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.013360 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:21:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.088015 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:40 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:21:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:21:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.034685 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' False 0.4044792917812593 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:21:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:21:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.034915 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:21:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:21:54 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: list index out of range -> Imputer mean # FeatureAgglomeration euclidean False 51 # SAM* SVC poly 3 0.010632086351533367 -1 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:21:59 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.030559 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:21:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:22:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.028522 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:22:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:22:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:03 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 8 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:22:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:22:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.011406 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:22:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:22:11 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:22:11 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:22:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:22:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:12 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:22:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:22:12 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:22:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:14 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:22:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:22:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.073997 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:22:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.001166 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:22:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.020978 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:22:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.076821 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:22:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:21 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:22:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:22:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:22:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.015895 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:22:22 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:22:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:22:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:22:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.011639 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:22:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:22:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:31 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.011725 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:22:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:41 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:22:41 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:22:41 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:22:41 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.015234 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:22:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:22:44 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:22:44 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:32:15 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-27 17:32:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:32:15 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:32:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 96 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete True 332 # SAM* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 True 220 True 0.6564306719064884 3 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete True 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.094184 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:32:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:85: RuntimeWarning: invalid value encountered in divide
  w1 /= np.sqrt((w1 ** 2).sum())

2018-02-27 17:32:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: array must not contain infs or NaNs -> Imputer median # FastICA deflation logcosh 100 0.099732 False 100 # SAM* ExtraTreesClassifier gini False False balanced 100 True 0.9457745734341919 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:32:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:32:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 10 0.050566 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:27 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:32:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 18 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:31 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 55 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:33 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '54.275042925685625' -> Imputer most_frequent # MinMaxScaler # SelectPercentile 54.275042925685625 f_classif # SAM* RandomForestClassifier gini True False balanced_subsample 100 False 0.8138233157708883 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete False 173 #  SAM*' RandomForestClassifier entropy True False None 100 False 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.047100 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 25 False hinge False None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:32:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:32:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.071135 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:32:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:32:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:85: RuntimeWarning: invalid value encountered in divide
  w1 /= np.sqrt((w1 ** 2).sum())

2018-02-27 17:32:44 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: array must not contain infs or NaNs -> Imputer median # FastICA deflation logcosh 100 0.093575 False 100 # SAM* ExtraTreesClassifier gini False False balanced 100 True 0.9457745734341919 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:32:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.079815 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:32:47 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:32:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:32:50 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:32:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:32:50 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:32:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 56 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:32:50 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:32:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 28 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete True 332 # SAM* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 auto 220 True 0.6564306719064884 3 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:32:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.020160 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:52 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:32:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:32:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:32:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:32:54 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:32:55 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.038472 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:32:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:32:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:32:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:32:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 52 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.051412 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:32:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:32:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:32:59 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 49 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:59 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete True 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.055829 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:59 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete False 173 #  SAM*' RandomForestClassifier entropy True True None 100 True 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:32:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 77 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:33:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.090435 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete False 173 #  SAM*' RandomForestClassifier entropy True False balanced 100 False 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:08 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:33:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:33:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:11 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:33:11 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 68 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.030474 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:12 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:33:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 62 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:33:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete False 332 # SAM* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 False 220 False 0.6564306719064884 3 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:13 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:33:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:33:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete True 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.019751 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete True 173 #  SAM*' RandomForestClassifier entropy True True balanced_subsample 100 True 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 750 False hinge True None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '54.275042925685625' -> Imputer most_frequent # MinMaxScaler # SelectPercentile 54.275042925685625 f_classif # SAM* RandomForestClassifier gini True False balanced_subsample 100 True 0.8138233157708883 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.023666 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:33:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '54.275042925685625' -> Imputer most_frequent # MinMaxScaler # SelectPercentile 54.275042925685625 f_classif # SAM* RandomForestClassifier gini True True balanced_subsample 100 False 0.8138233157708883 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:22 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 17:33:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.006599 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 50 True hinge True None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:27 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:33:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:33:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:33:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.034807 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 10 False hinge True None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:28 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:33:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 11 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete True 173 #  SAM*' RandomForestClassifier entropy True False balanced_subsample 100 False 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.002387 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:28 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:33:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:33:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:33:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:32 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:33:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 50 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:32 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 17:33:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 59 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 33 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:33:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:33:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:85: RuntimeWarning: invalid value encountered in divide
  w1 /= np.sqrt((w1 ** 2).sum())

2018-02-27 17:33:39 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: array must not contain infs or NaNs -> Imputer median # FastICA deflation logcosh 100 0.003007 False 100 # SAM* ExtraTreesClassifier gini False False None 100 False 0.9457745734341919 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:33:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:41 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 750 0.035602 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:43 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:33:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:33:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:33:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:85: RuntimeWarning: invalid value encountered in divide
  w1 /= np.sqrt((w1 ** 2).sum())

2018-02-27 17:33:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: array must not contain infs or NaNs -> Imputer median # FastICA deflation logcosh 100 0.055082 False 100 # SAM* ExtraTreesClassifier gini False False balanced 100 True 0.9457745734341919 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete True 173 #  SAM*' RandomForestClassifier entropy True True balanced_subsample 100 False 0.5565918060287016 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:48 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:33:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:33:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:33:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.058790 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:33:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 250 True hinge False None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:52 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 17:33:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete True 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.079087 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:33:56 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:33:56 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 38 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:56 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '54.275042925685625' -> Imputer most_frequent # MinMaxScaler # SelectPercentile 54.275042925685625 f_classif # SAM* RandomForestClassifier gini True True None 100 False 0.8138233157708883 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:57 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:33:57 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:33:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.037569 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.021512 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:33:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:33:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.010081 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:33:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:33:59 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 61 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:33:59 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:34:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:34:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.024179 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:03 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:34:03 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 58 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:03 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.016541 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:34:04 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:34:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:34:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete True 173 #  SAM*' RandomForestClassifier entropy True True None 100 False 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:07 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:34:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:34:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:34:11 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete False 173 #  SAM*' RandomForestClassifier entropy True True None 100 False 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:11 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '54.275042925685625' -> Imputer most_frequent # MinMaxScaler # SelectPercentile 54.275042925685625 f_classif # SAM* RandomForestClassifier gini True True None 100 True 0.8138233157708883 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:12 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete False 177 # SAM* SVC rbg 10 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.005652 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete False 173 #  SAM*' RandomForestClassifier entropy True False balanced_subsample 100 True 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:14 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:34:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 83 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete True 332 # SAM* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 False 220 False 0.6564306719064884 3 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:34:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete True 332 # SAM* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 False 220 True 0.6564306719064884 3 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:34:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:34:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:15 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:34:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:34:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:34:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:34:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:34:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete True 173 #  SAM*' RandomForestClassifier entropy True True balanced 100 True 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:24 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:34:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 90 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.016048 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 50 False hinge False None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.064307 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:24 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:34:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:34:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete True 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.031162 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:24 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:34:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 60 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.069376 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.052638 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:34:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:30 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:34:30 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:34:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:34:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:34:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:34:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:34:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete True 173 #  SAM*' RandomForestClassifier entropy True False balanced 100 True 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete True 173 #  SAM*' RandomForestClassifier entropy True True None 100 True 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:34:42 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 94 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 177 clusters where given for a tree with 8 leaves. -> Imputer mean # FeatureAgglomeration cosine complete True 177 # SAM* SVC rbg 3 6.58869648864534e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.038686 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.004682 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:46 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete True 173 #  SAM*' RandomForestClassifier entropy True False balanced 100 False 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:46 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.003129 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:34:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:34:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.038038 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:49 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:34:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 16 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.081532 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.060258 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 22 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:34:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:57 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:34:57 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 42 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:34:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:34:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.090086 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:35:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:35:09 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 17:35:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.063432 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:09 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 17:35:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 100 False hinge False None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:35:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:35:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:35:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:35:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:35:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:21 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 750 True hinge True None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:21 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:35:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 1000 0.021398 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.030646 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:23 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.082208 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:35:25 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:35:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:35:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 50 False hinge True None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 50 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.050843 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 10 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:29 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.047051 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:29 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 17:35:29 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.072720 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:29 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 750 False hinge False None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:29 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete True 173 #  SAM*' RandomForestClassifier entropy True True balanced_subsample 100 False 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:35:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '54.275042925685625' -> Imputer most_frequent # MinMaxScaler # SelectPercentile 54.275042925685625 f_classif # SAM* RandomForestClassifier gini True True balanced 100 True 0.8138233157708883 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:85: RuntimeWarning: invalid value encountered in divide
  w1 /= np.sqrt((w1 ** 2).sum())

2018-02-27 17:35:35 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: array must not contain infs or NaNs -> Imputer median # FastICA deflation logcosh 100 0.096701 False 100 # SAM* ExtraTreesClassifier gini False False None 100 True 0.9457745734341919 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:35:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:39 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 27 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:39 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:35:39 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 47 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:41 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete True 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.049038 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:41 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 85 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:46 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:35:46 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 39 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:35:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:50 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:35:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:35:50 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:35:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:35:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 18 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.003508 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:35:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.054538 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete False 173 #  SAM*' RandomForestClassifier entropy True False None 100 True 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:51 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:35:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:35:51 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '54.275042925685625' -> Imputer most_frequent # MinMaxScaler # SelectPercentile 54.275042925685625 f_classif # SAM* RandomForestClassifier gini True True balanced 100 False 0.8138233157708883 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 5 True hinge True None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:52 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete False 332 # SAM* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 True 220 False 0.6564306719064884 3 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:55 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.090856 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:35:55 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:35:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:35:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:35:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:35:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:35:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:35:58 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:35:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:35:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:00 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:36:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 94 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:05 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:36:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.063645 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:36:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 250 0.030387 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:07 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:36:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:36:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:36:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.034812 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.095269 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 100 0.030677 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 46 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:10 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:36:11 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.008086 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:36:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete False 332 # SAM* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 True 220 True 0.6564306719064884 3 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:36:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:17 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:36:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:36:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:36:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.096694 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 47 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete True 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.027343 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:21 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.086359 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:36:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.082136 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 100 False hinge True None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete True 332 # SAM* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 True 220 True 0.6564306719064884 3 0.0 None
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:36:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 26 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:36:28 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:29 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:36:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:36:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:31 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.024618 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 auto 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:31 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.013307 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:31 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:36:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 35 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:33 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.024336 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:33 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:36:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:36:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:36:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:36:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:40 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:36:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 34 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:41 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:36:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:36:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:36:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.089634 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete True 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.067969 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete False 332 # SAM* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 auto 220 True 0.6564306719064884 3 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:45 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:36:45 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 70 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:45 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:36:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:36:45 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.022417 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:49 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:50 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:36:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:36:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:53 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:53 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete False 332 # SAM* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 auto 220 False 0.6564306719064884 3 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:53 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete True 332 # SAM* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 auto 220 False 0.6564306719064884 3 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:53 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.017655 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:55 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 17:36:55 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.018520 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:55 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 63 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:55 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:36:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:36:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.046438 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:36:58 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:36:59 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:37:00 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:37:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:37:00 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:37:01 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:02 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:37:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:37:02 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:37:03 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 36 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:03 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:04 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 17:37:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 28 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 72 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 500 True hinge True None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:04 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:37:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:37:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 25 True hinge True None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:05 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:37:05 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:37:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:37:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:37:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:37:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 7 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete False 173 #  SAM*' RandomForestClassifier entropy True True balanced 100 True 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:06 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:07 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:08 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:37:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.034592 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 82 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 5 False hinge True None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 1000 0.053925 False 3 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 False 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 20 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:09 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:11 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:12 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:37:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:37:12 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:14 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:37:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 54 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:37:14 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:37:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:37:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:15 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '54.275042925685625' -> Imputer most_frequent # MinMaxScaler # SelectPercentile 54.275042925685625 f_classif # SAM* RandomForestClassifier gini True False None 100 True 0.8138233157708883 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 14 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '54.275042925685625' -> Imputer most_frequent # MinMaxScaler # SelectPercentile 54.275042925685625 f_classif # SAM* RandomForestClassifier gini True False balanced 100 True 0.8138233157708883 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:17 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:37:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 500 False hinge True None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:18 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:19 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:37:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 51 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:19 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:20 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:37:21 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.060800 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:37:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:24 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 100 True hinge False None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:24 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:25 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-27 17:37:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 91 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.017071 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete True 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.014635 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:25 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:26 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:27 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 10 False hinge False None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:27 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:85: RuntimeWarning: invalid value encountered in divide
  w1 /= np.sqrt((w1 ** 2).sum())

2018-02-27 17:37:31 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: array must not contain infs or NaNs -> Imputer median # FastICA deflation logcosh 100 0.080417 False 100 # SAM* ExtraTreesClassifier gini False False balanced_subsample 100 True 0.9457745734341919 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:31 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-27 17:37:31 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-27 17:37:31 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.092596 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:33 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 500 0.092130 False 2 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 True 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 92 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:34 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 60 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:37:35 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 65 1 euclidean
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '54.275042925685625' -> Imputer most_frequent # MinMaxScaler # SelectPercentile 54.275042925685625 f_classif # SAM* RandomForestClassifier gini True False balanced 100 False 0.8138233157708883 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:37 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:38 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:39 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-27 17:37:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-27 17:37:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:37:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 10 0.044544 False 100 # SAM* ExtraTreesClassifier entropy True False balanced_subsample 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:40 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:41 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:42 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:42 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 86 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-27 17:37:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-27 17:37:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-27 17:37:44 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-27 17:37:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-27 17:37:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:36:44 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-02-28 14:36:45 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:36:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:36:46 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:36:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:36:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:36:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:36:47 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:36:47 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:36:47 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:36:47 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:36:47 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:36:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:36:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:36:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:36:47 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:36:47 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:36:47 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:36:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:36:47 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:36:47 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # RobustScaler True True # SelectFwe 0.39376071555683756 chi2 # SAM* SVC rbf 3 2.3839685780861314e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:36:47 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-02-28 14:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:36:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:36:49 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:36:49 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:36:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:36:49 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:36:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:36:49 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:36:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:36:49 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:36:49 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:36:49 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:36:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-28 14:37:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:30 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:37:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:31 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:31 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:37:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:32 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-28 14:37:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 60 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:37:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:32 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:37:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)

2018-02-28 14:37:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:37:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:37:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> MaxAbsScaler # SparseRandomProjection 0.541262 False 0.399946 2 # SA*  MultinomialNB 1.930184 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:37:36 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:37:36 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:37:36 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> RobustScaler False True # SelectFwe 0.615477 f_classif # PolynomialFeatures 6 True True # SA*  RidgeCCV None 2.186524 None False True # RadiusNeighborsClassifier 28.512295 uniform ball_tree 73 4 manhattan # GaussianNB # Perceptron l1 5.091589 False 10 False 0.528843 balanced True # BernoulliNB 0.619040 2.853854 True # MultinomialNB 4.196781 True # PassiveAggressive  False 50 False squared_loss True None # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:37:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:36 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:36 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:37:36 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:37:39 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:37:39 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:40 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:40 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:37:43 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:43 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:37:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 14:37:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 14:37:43 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:37:43 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-28 14:37:44 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:37:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:49 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:49 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:49 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:37:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:49 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:49 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:37:49 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:49 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:49 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:37:49 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:37:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:50 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:50 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:50 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:51 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:37:51 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:37:51 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:51 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:52 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)

2018-02-28 14:37:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:52 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:53 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:53 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:53 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:53 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:37:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:54 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:54 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:37:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:54 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:37:54 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:37:54 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-28 14:37:58 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' False 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:37:58 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-28 14:37:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:58 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:37:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:37:59 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> RobustScaler False True # SA*  LogisticCV 2 False 450 liblinear None False 0.029935 # RidgeClassifier 1000 False svd 0.050704 3.642024 balanced False False # BernoulliNB 0.272620 8.458216 True # QuadraticDiscriminantAnalysis 0.363860 0.078170 False # RidgeCCV 3 6.847025 None True True # Perceptron l2 8.747352 True 50 False 0.522902 None False # MultinomialNB 7.532114 True # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:37:59 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:38:37 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-28 14:38:37 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer median # SelectFromModel 0.421906 False # PolynomialFeatures 5 False False # SA*  MultinomialNB 2.690421 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:38:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:38 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:38:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:38:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:38:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:39 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:38:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:38:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:38:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:38:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:38:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:38:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:38:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:38:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:38:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:38:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:38:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:38:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:38:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:38:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:38:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:38:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:38:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.043009 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:38:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:40 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:38:41 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:38:41 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:38:41 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:38:42 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:38:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:43 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:38:43 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:38:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:43 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:38:43 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:38:43 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:38:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:43 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:38:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:43 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:38:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:43 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:38:44 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:45 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:38:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:45 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:38:45 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:38:45 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:38:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:46 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:38:46 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:38:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:38:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:46 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 14:38:46 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:38:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-28 14:38:46 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:85: RuntimeWarning: invalid value encountered in divide
  w1 /= np.sqrt((w1 ** 2).sum())

2018-02-28 14:38:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: array must not contain infs or NaNs -> Imputer median # FastICA deflation logcosh 100 0.075698 False 100 # SAM* ExtraTreesClassifier gini False False balanced 100 False 0.9457745734341919 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:38:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:38:50 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:38:50 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:50 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:38:50 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:38:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:38:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:38:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:38:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:52 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:38:52 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:38:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:52 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:38:53 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:38:53 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:38:53 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:38:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:54 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:38:54 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:38:58 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:38:58 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:38:58 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:38:59 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:38:59 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:39:00 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:06 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:39:06 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 14:39:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:06 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:39:06 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:39:07 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:07 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:07 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:39:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:08 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:39:09 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer mean # StandardScaler True False # SelectPercentile 18 chi2 # PolynomialFeatures 4 True False # SA*  GradientBoostingClassifier exponential 0.054378 0.041077 True 45 False sqrt 65821 0.119384 68681 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:39:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:10 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:11 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:11 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:39:11 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:12 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:39:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:12 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:12 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:12 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:39:13 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:39:13 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:13 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:13 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:15 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:39:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:16 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:16 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:39:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:39:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 14:39:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:39:17 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:39:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:17 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:39:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:17 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:39:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:18 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:39:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> RobustScaler True True # SelectKBest 3 chi2 # PolynomialFeatures 10 False False # SA*  Perceptron l1 1.968134 True 500 True 0.653789 balanced False # SVC rbf 5 0.019563 1000 balanced # AdaBoostClassifier SAMME 20 1.308442 # Stacking 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:39:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:21 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:39:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:21 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:22 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:22 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-28 14:39:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:22 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> RobustScaler False True # SelectFwe 0.733892 f_classif # SA*  ExtraTreeClassifier gini random None False log2 None 0.122478 None # DecisionTreeClassifier entropy best balanced False 0.238361 90063 0.293482 None # GaussianNB # RadiusNeighborsClassifier 7.633566 uniform ball_tree 40 15 minkowski # KNeighborsClassifier 27 uniform auto 98 9 minkowski # SVC linear 8 0.092531 500 balanced # MultinomialNB 2.447395 False # Perceptron l2 5.045849 True 750 True 0.524382 balanced True # LinearDiscriminantAnalysis svd 0.084148 False # Perceptron l1 4.233735 True 25 False 0.553832 None True # LinearDiscriminantAnalysis lsqr 0.030764 False # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:39:22 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:22 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:39:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:23 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:39:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:23 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:39:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:24 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:39:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:24 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:39:24 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:39:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:39:24 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:39:25 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:40:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:01 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:40:01 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:40:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:04 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:40:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:04 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:40:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:04 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:40:05 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:40:05 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-28 14:40:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:05 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:40:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:06 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:40:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:06 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-28 14:40:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 55 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:40:06 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:40:06 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:40:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:07 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:40:07 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:40:07 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:40:07 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:40:07 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:40:07 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:40:07 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:40:07 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:40:07 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:40:07 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:40:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:07 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:40:07 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:40:07 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:40:08 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:40:08 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-28 14:40:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer most_frequent # Normalizer max # SelectFwe 0.698184 chi2 # SA*  BernoulliNB 0.143735 7.761086 True # SVC rbf 8 0.052076 500 balanced # Perceptron l1 8.955771 True 50 True 0.015099 balanced False # MultinomialNB 1.995990 True # RidgeClassifier 500 True sparse_cg 0.064756 3.381127 None False False # DecisionTreeClassifier gini best balanced True sqrt None 0.219180 5264 # DecisionTreeClassifier entropy random balanced False None None 0.127408 11507 # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:40:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:08 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:40:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:40:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 5 False hinge False None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:40:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:09 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:09 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:09 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:40:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:10 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:40:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:11 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:40:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:40:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:40:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:40:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:13 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:40:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:13 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:40:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:40:45 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:40:45 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:40:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn("Singular matrix in solving dual problem. Using "

2018-02-28 14:40:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:40:54 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:40:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:00 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:41:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:00 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:41:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:01 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 14:41:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:41:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:41:01 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 14:41:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:01 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:41:01 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:41:01 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:41:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:01 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:41:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:41:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:41:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:02 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 14:41:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:02 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:41:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:41:02 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:41:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:41:02 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:41:03 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:41:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:03 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:41:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:03 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:41:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:41:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:03 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:41:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:03 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:41:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:04 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:41:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:05 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:41:05 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:41:05 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:41:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:05 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:41:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:05 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:41:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:06 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:41:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:06 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:41:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:19 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:41:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:41:19 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:41:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:20 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:41:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:41:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:20 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:41:20 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:41:20 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:41:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:22 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:41:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:41:22 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:41:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:41:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:22 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:41:22 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:41:22 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:41:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:41:22 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:41:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:41:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:41:24 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:41:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:41:29 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:41:50 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:41:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:41:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:42:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:42:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:42:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:42:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:42:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:42:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:43:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:43:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:43:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:43:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:43:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:43:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 14:44:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:44:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:44:31 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:44:31 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:44:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:44:31 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:44:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:44:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:44:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:44:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:44:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:44:33 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:44:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:44:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:44:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:44:33 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:44:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:44:33 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:44:33 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:44:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:44:50 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:44:50 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:44:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:44:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:44:50 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:44:51 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:44:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:44:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:44:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:44:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:44:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:44:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:44:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:44:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-28 14:44:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:44:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:44:51 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:44:52 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:44:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:44:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:44:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:44:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:44:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:44:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:44:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:44:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:44:52 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:44:52 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:44:52 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:44:52 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:45:35 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:45:35 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:45:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:45:42 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 14:45:42 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:45:42 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:45:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:45:42 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:45:43 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:45:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:45:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:45:43 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:45:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:45:43 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:45:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:45:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 auto 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:45:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:45:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:45:44 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:45:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:45:59 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:45:59 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:45:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:45:59 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:45:59 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:00 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:00 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:46:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:00 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:00 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:00 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:00 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:02 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:03 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:03 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:46:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:03 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:04 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:04 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:04 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:04 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:46:04 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:05 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:05 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 6.17340339792e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 14:46:05 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 5.92867135263e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 14:46:05 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 3.65078232655e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 14:46:05 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 2.47675637918e-20 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 14:46:06 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 14:46:06 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:46:06 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:46:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:07 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:46:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete False 173 #  SAM*' RandomForestClassifier entropy True False balanced_subsample 100 True 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:46:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:07 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:46:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:08 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:46:09 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:46:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:10 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:10 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete True 332 # SAM* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 True 220 True 0.6564306719064884 3 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:46:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:10 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:11 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:46:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:11 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:46:11 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:46:11 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:11 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:11 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:46:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:12 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:12 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:12 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:12 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:13 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.090423 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:46:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:13 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:13 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:13 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 14:46:13 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:13 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:14 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:46:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:14 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:46:14 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:14 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:14 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:46:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:15 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:16 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:46:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:46:16 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:46:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:46:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:46:17 : /usr/local/lib/python2.7/dist-packages/scipy/optimize/linesearch.py:461: LineSearchWarning: The line search algorithm did not converge
  warn('The line search algorithm did not converge', LineSearchWarning)

2018-02-28 14:46:17 : /usr/local/lib/python2.7/dist-packages/scipy/optimize/linesearch.py:312: LineSearchWarning: The line search algorithm did not converge
  warn('The line search algorithm did not converge', LineSearchWarning)

2018-02-28 14:46:18 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.
  "number of iterations.", ConvergenceWarning)

2018-02-28 14:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:49:01 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:49:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:01 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 14:49:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:01 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:49:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:02 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:49:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:02 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:49:02 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 14:49:02 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 14:49:02 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:02 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:49:02 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:02 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> Imputer median # StandardScaler True True # SA*  PassiveAggressive  False 100 False epsilon_insensitive True balanced # RidgeCCV 9 8.971695 None True True # LinearDiscriminantAnalysis svd 0.064876 True # NuSVC sigmoid 6 0.087234 100 None # LogisticRegression True 350 liblinear None False 0.031238 # RidgeClassifier 100 True cholesky 0.086712 7.934243 None False True # ExtraTreeClassifier entropy random None auto 0.397730 17852 0.393888 None # RadiusNeighborsClassifier 27.864966 distance auto 74 10 manhattan # SVC rbf 5 0.058470 500 None # PassiveAggressive  True 50 False modified_huber False balanced # Perceptron l1 3.200149 True 25 True 0.417889 balanced True # RidgeCCV None 0.599934 None True False # NuSVC poly 7 0.068272 500 balanced # MultinomialNB 3.258052 True # DecisionTreeClassifier gini best balanced False None 29425 0.047455 60561 # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:49:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:49:03 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:03 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:49:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:03 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:03 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:49:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:03 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:49:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:04 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:49:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:49:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:04 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:49:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:04 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 14:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:07 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> FastICA deflation logcosh 1000 0.058259 True 3 # PolynomialFeatures 8 False False # SA*  LinearDiscriminantAnalysis svd 0.066115 True # RidgeClassifier 500 False lsqr 0.036993 0.688571 balanced True False # DecisionTreeClassifier entropy best None auto sqrt None 0.176603 85286 # ExtraTreeClassifier entropy best balanced True 0.867719 None 0.002003 28927 # ExtraTreeClassifier entropy random balanced auto 0.546925 36664 0.200552 None # RidgeClassifier 500 True svd 0.017142 4.126195 balanced True False # GaussianNB # Perceptron l2 1.142194 False 25 True 0.747104 None False # SVC linear 6 0.058804 10 balanced # SVC linear 5 0.084037 10 balanced # KNeighborsClassifier 16 distance ball_tree 61 9 chebyshev # DecisionTreeClassifier entropy random balanced True sqrt 77979 0.266058 None # MultinomialNB 0.964893 True # RadiusNeighborsClassifier 20.018400 uniform brute 95 10 chebyshev # MultinomialNB 2.055547 False # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:26 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:49:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:49:26 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:26 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:49:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:26 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:49:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:28 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:49:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:28 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:49:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:49:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:30 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:30 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-28 14:49:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-02-28 14:49:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-28 14:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:34 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:49:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:34 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:49:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:49:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:49:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:49:35 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:49:35 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:49:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:00 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:01 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:50:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:02 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:50:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:03 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:50:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:03 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:50:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:50:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:04 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:04 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:05 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:50:05 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:05 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:05 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:50:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:50:05 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:50:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:09 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:09 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:50:09 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:50:09 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:10 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:11 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:11 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:11 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:50:11 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:50:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:11 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:11 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:12 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:50:12 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:50:12 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:12 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:12 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:50:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:14 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:50:14 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:14 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:15 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:50:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:15 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:21 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:50:21 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:22 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:50:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:23 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:50:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:24 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:50:25 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:50:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:25 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:50:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:26 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:26 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:26 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:26 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:26 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:26 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:27 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:50:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:27 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:27 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:28 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:28 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:50:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:28 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:50:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:30 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:30 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:50:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:50:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:50:30 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:50:38 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:51:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:04 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.001084 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 53 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:51:05 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:05 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:51:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:05 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:51:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:05 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:51:05 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:51:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:05 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:51:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:06 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:06 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:51:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:06 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:51:07 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer mean # RobustScaler True False # SelectFdr 0.582525 chi2 # SA*  NuSVC linear 2 0.080776 500 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 14:51:08 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:08 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:51:09 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:09 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:51:09 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:09 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:09 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:09 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:09 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 14:51:09 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:51:10 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 14:51:10 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:51:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:10 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:51:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:10 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:51:11 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:11 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:11 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:51:12 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 14:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:20 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:51:20 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 14:51:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:20 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:51:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:21 : /usr/local/lib/python2.7/dist-packages/scipy/optimize/linesearch.py:461: LineSearchWarning: The line search algorithm did not converge
  warn('The line search algorithm did not converge', LineSearchWarning)

2018-02-28 14:51:21 : /usr/local/lib/python2.7/dist-packages/scipy/optimize/linesearch.py:312: LineSearchWarning: The line search algorithm did not converge
  warn('The line search algorithm did not converge', LineSearchWarning)

2018-02-28 14:51:23 : /usr/local/lib/python2.7/dist-packages/scipy/optimize/linesearch.py:421: LineSearchWarning: Rounding errors prevent the line search from converging
  warn(msg, LineSearchWarning)

2018-02-28 14:51:23 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed
  warnings.warn('Line Search failed')

2018-02-28 14:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:50 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:51:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:52 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 14:51:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:52 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:51:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 14:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 14:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:54 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:51:54 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 14:51:54 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 14:51:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 14:59:00 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:01:41 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:01:47 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:01:49 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:01:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:01:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:01:57 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:02:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:02:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:02:27 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:02:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:02:46 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:02:46 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 15:02:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:02:48 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer mean # StandardScaler True True # SelectPercentile 62 chi2 # SA*  GaussianNB # RidgeClassifier 500 True sparse_cg 0.044064 3.747897 balanced False False # SVC poly 10 0.001818 10 balanced # RidgeClassifier 100 True sparse_cg 0.089380 8.609612 balanced False False # Perceptron l2 8.486397 False 250 True 0.691667 balanced False # NuSVC sigmoid 7 0.083584 100 None # SVC sigmoid 10 0.057568 100 None # Perceptron l2 0.882161 False 10 False 0.871819 balanced True # DecisionTreeClassifier entropy best balanced True sqrt None 0.007176 None # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 15:02:48 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:02:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:03:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete True 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.099188 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 15:03:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:03:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:03:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:03:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:03:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:03:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:03:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:03:37 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:03:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:03:49 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:03:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:03:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:03:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:03:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:03:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:03:54 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:03:54 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:03:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:03:54 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:03:54 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:03:55 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:03:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:03:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:03:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:03:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:03:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:04 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:04:04 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:05 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:07 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 4.27142421113e-20 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:04:08 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 6.20028893009e-20 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:04:08 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 4.56264291364e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:04:08 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 5.77183220675e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:04:08 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 4.53176850321e-18 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:04:08 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 5.5710097221e-18 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:04:09 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 5.34448471418e-21 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:04:09 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 6.90720460454e-21 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:04:09 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 1.77696984587e-21 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:04:09 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 1.74945923158e-21 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:04:09 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 2.55374938077e-18 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:04:09 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:14 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:16 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:04:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:16 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:16 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> Imputer median # StandardScaler True True # PolynomialFeatures 7 True True # SA*  MultinomialNB 5.901474 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 15:04:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:17 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:04:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:04:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:04:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:21 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:24 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:24 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:24 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:25 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:25 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:25 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:26 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:27 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 15:04:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:28 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:28 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:04:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:28 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:04:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 15:04:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:31 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:04:32 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:04:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:04:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.
  "number of iterations.", ConvergenceWarning)

2018-02-28 15:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:35 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:04:35 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 15:04:35 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:35 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:04:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:35 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:35 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 15:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:04:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:05:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:05:09 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:05:09 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:05:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:05:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:05:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:05:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:06:09 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:06:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:06:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:06:10 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:06:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:06:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:06:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:06:10 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:06:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:06:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:06:10 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:06:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:06:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:06:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:07:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:07:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:07:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:07:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:07:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:07:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:07:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:07:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:07:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:08:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:08:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:08:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:09:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # StandardScaler True False # SelectFpr 0.701704 chi2 # SA*  DecisionTreeClassifier entropy random None False None None 0.326174 None # PassiveAggressive  False 500 True perceptron True balanced SA*  LogisticRegression True 450 liblinear None False 0.094066 # NuSVC linear 3 0.047035 500 balanced # SVC sigmoid 2 0.029940 500 None # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 15:09:05 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:06 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:09:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:06 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:09:07 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:09:15 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:09:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:09:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:09:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:09:15 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:16 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:09:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:17 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:09:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)

2018-02-28 15:09:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:19 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> MaxAbsScaler # FastICA deflation logcosh 500 0.051341 True 3 # PolynomialFeatures 4 True False # SA*  GaussianNB # BernoulliNB 0.172918 1.038788 False SA*  DecisionTreeClassifier entropy random balanced False log2 None 0.220375 71106 # MultinomialNB 5.863529 True # Perceptron l2 8.533503 False 100 True 0.015920 balanced False # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 15:09:19 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:09:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:20 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:09:20 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:09:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 15:09:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:20 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:09:20 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:09:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:21 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 15:09:21 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 15:09:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:21 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:09:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:22 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:09:22 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:09:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:24 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:09:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:24 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:09:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:25 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:25 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:09:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:26 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:09:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:27 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:27 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:09:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:27 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:09:27 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:09:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:27 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:27 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:27 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:09:27 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:09:27 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:28 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 15:09:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:28 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:09:29 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:09:29 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 15:09:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:09:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:09:29 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:09:29 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> StandardScaler True True # SA*  MultinomialNB 7.261484 False # BernoulliNB 0.993118 0.918287 False # RandomForestClassifier entropy False False balanced_subsample 35 True sqrt 50979 0.277821 None # MultinomialNB 8.060158 False # QuadraticDiscriminantAnalysis 0.071616 0.098925 False # Stacking 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 15:09:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:09:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:09:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:09:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:09:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:09:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:09:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:10:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:10:21 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:10:21 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:10:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:10:22 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:10:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:10:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:11:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:11:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:11:08 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:11:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:11:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:11:08 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:11:08 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:11:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:11:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> IncrementalPCA False 2 # PolynomialFeatures 4 True True # SA*  NuSVC rbf 7 0.061963 1000 None # BernoulliNB 0.010494 8.056846 False # RidgeCCV None 4.302190 balanced False False # DecisionTreeClassifier gini random None auto 0.677331 67709 0.439571 79046 # DecisionTreeClassifier gini random None False None 40488 0.211745 9744 # RidgeCCV None 0.485114 balanced True False # MultinomialNB 1.895106 True # QuadraticDiscriminantAnalysis 0.774389 0.036955 False # DecisionTreeClassifier gini best None auto log2 None 0.436060 80916 # PassiveAggressive  True 750 True epsilon_insensitive True balanced # RadiusNeighborsClassifier 1.052400 distance ball_tree 65 9 minkowski # GaussianNB # PassiveAggressive  False 25 False perceptron True balanced # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 15:11:09 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:11:09 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:11:09 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:11:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:11:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:11:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:11:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 15:11:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:11:10 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:11:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:11:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:11:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:11:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 15:11:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:11:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:11:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:11:11 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:11:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:11:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 15:11:12 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:11:54 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:12:28 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:12:28 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:12:28 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:12:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:12:29 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:12:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:12:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:12:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:12:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:12:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:12:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:12:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:12:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:12:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:12:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:12:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:00 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-28 15:13:00 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer most_frequent # StandardScaler False False # SelectFromModel 0.959336 False # SA*  KNeighborsClassifier 6 uniform kd_tree 27 5 chebyshev 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 15:13:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:01 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:01 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:01 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:01 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:01 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:13:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:01 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:13:01 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:01 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:13:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:03 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:13:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:04 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:13:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:04 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.
  "number of iterations.", ConvergenceWarning)

2018-02-28 15:13:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer most_frequent # RobustScaler False True # SelectFpr 0.071418 chi2 # PolynomialFeatures 2 False False # SA*  SVC rbf 8 0.051211 1000 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 15:13:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:05 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:05 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:05 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:05 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:06 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:07 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:13:07 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:07 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:07 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:08 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:09 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:09 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-02-28 15:13:09 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:09 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:09 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:09 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:09 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:09 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 True 480 False 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 15:13:10 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:10 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:10 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:11 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:12 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:12 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:12 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:12 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:14 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:14 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:14 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 15:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 15:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:19 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 15:13:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:24 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:24 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:13:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:24 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:24 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:24 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:24 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:24 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:13:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:25 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:25 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:25 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:25 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:26 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:26 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:26 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:26 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:26 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:26 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:26 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:27 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:13:27 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:27 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:27 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:27 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 15:13:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:27 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:27 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:30 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:32 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:13:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:32 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:32 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:33 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete False 332 # SAM* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 True 220 True 0.6564306719064884 3 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 15:13:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-02-28 15:13:34 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:35 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:35 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:13:35 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 15:13:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:35 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:36 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:36 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 3.01117534274e-30 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:13:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:37 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:37 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:37 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:37 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:13:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn("Singular matrix in solving dual problem. Using "

2018-02-28 15:13:39 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 3.03749933267e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:13:39 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 1.17787255961e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:13:39 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 2.9298098062e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:13:39 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 1.00221003404e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:13:39 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 5.12222184053e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:13:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:40 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:40 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:40 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:40 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:13:40 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 15:13:40 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:41 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:13:41 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:41 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:44 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:44 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:13:44 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-02-28 15:13:46 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Normalizer l2 # SelectFwe 0.835548 chi2 # SA*  NuSVC sigmoid 2 0.022524 500 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:13:46 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:13:47 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:13:50 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:16:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:28 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:16:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:28 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:16:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:30 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:30 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:16:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.
  "number of iterations.", ConvergenceWarning)

2018-02-28 15:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 15:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:16:41 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:41 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:41 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 15:16:41 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:41 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:41 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:16:44 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:44 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:16:44 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 15:16:44 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.061576 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 15:16:45 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:16:45 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:16:45 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:16:45 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:16:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:16:45 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:16:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:16:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:16:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:46 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:16:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:16:47 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:47 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> RobustScaler False False # SA*  KNeighborsClassifier 24 distance kd_tree 64 14 minkowski # LogisticCV 4 False 150 newton-cg None False 0.003577 # NuSVC poly 2 0.080687 1000 None # MultinomialNB 5.317044 True # PassiveAggressive  True 100 True log True None # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 15:16:47 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:16:47 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:16:47 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:16:47 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:16:47 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:47 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:48 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:48 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:50 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:16:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:16:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:51 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:51 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:16:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:16:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:16:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:16:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:16:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:51 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:16:51 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:51 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:52 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 2.10033965379e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-02-28 15:16:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:52 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:52 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:52 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:52 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:16:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:16:52 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:52 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:16:52 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:16:52 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:16:53 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:16:53 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:16:53 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:17:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:33 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:17:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:17:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:34 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:34 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:17:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:35 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:35 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:17:35 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:35 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:17:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:35 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:17:35 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 15:17:35 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:35 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:35 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:35 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:17:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:36 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:17:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:37 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:37 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:17:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:17:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:17:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:17:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:17:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:38 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:17:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:38 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:17:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:17:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:17:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:40 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:41 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:17:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:17:41 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:17:42 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:17:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:43 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:17:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:17:44 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:44 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 15:17:44 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:17:44 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for float(): 1..0 -> Imputer mean # MinMaxScaler # SAM* DecisionTreeClassifier entropy best None auto 1..0 0.24229264852063404 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 15:17:44 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:45 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:57 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:57 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:57 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:57 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:57 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:57 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:17:57 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:57 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:17:57 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:57 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 15:17:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:58 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:58 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:58 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:17:58 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:17:58 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:17:58 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:17:58 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:17:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:59 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:17:59 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:59 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:17:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:17:59 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:59 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:17:59 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:18:00 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:18:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:18:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:18:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:18:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:18:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:18:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:18:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:18:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:18:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:18:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:18:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:18:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:18:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:18:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-02-28 15:19:22 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:19:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:19:22 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:19:24 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:19:24 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:19:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:19:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:19:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-02-28 15:19:25 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:19:25 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:19:25 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:19:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:19:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:19:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:19:25 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:19:27 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:19:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:19:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:19:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:19:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:19:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:19:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:19:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:19:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:19:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:19:30 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:19:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:19:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:19:30 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:19:31 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:19:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:19:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:19:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:19:31 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:19:34 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-02-28 15:19:35 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:19:35 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:19:35 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-02-28 15:19:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:19:35 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:19:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:19:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:19:35 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:19:35 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:19:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:19:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:19:35 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:19:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:19:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:19:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:19:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:19:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:19:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:19:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:19:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:19:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:19:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:19:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:19:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:20:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:20:59 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:21:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:21:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:21:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:21:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:21:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:21:00 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:21:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:00 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:21:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:00 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:21:00 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:21:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:01 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:21:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:21:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:21:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:01 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:21:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:21:02 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:21:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:21:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:02 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:21:02 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-02-28 15:21:03 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:21:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:21:04 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:21:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:21:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:21:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:21:05 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-02-28 15:21:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:05 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:21:05 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-02-28 15:21:05 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:21:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:05 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:21:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:05 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:21:05 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 750 0.016332 False 100 # SAM* ExtraTreesClassifier entropy True False None 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-02-28 15:21:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:21:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:21:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:06 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:21:08 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:21:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:08 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:21:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:08 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-02-28 15:21:08 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-02-28 15:21:08 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-02-28 15:21:08 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-02-28 15:21:08 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-02-28 15:21:08 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-02-28 15:21:08 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:305: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.
  warn("Warm-start fitting without increasing n_estimators does not "

2018-02-28 15:21:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-02-28 15:21:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:47:35 : /usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)

2018-03-01 17:47:35 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:47:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # RobustScaler True True # SelectFwe 0.39376071555683756 chi2 # SAM* SVC rbf 3 2.3839685780861314e-05 -1 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:47:36 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:301: UserWarning: n_components is too large: it will be set to 8
  warnings.warn('n_components is too large: it will be set to %s' % n_components)

2018-03-01 17:47:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:47:37 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:47:37 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:47:37 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:47:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:47:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:47:37 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:47:37 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:47:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:47:37 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:47:37 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:47:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:47:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:47:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:47:37 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:47:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:47:38 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:47:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:47:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:47:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-03-01 17:48:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:13 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:48:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:14 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:48:14 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:14 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:14 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-03-01 17:48:14 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 60 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:48:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:15 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:48:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)

2018-03-01 17:48:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:48:16 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:48:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> MaxAbsScaler # SparseRandomProjection 0.541262 False 0.399946 2 # SA*  MultinomialNB 1.930184 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:48:17 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:48:17 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:48:17 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> RobustScaler False True # SelectFwe 0.615477 f_classif # PolynomialFeatures 6 True True # SA*  RidgeCCV None 2.186524 None False True # RadiusNeighborsClassifier 28.512295 uniform ball_tree 73 4 manhattan # GaussianNB # Perceptron l1 5.091589 False 10 False 0.528843 balanced True # BernoulliNB 0.619040 2.853854 True # MultinomialNB 4.196781 True # PassiveAggressive  False 50 False squared_loss True None # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:48:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:18 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:18 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:48:18 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:48:20 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:48:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:21 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:21 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 17:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 17:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:48:23 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-03-01 17:48:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:25 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:48:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:27 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:28 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:48:28 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:48:28 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:28 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:30 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:30 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:48:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:30 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:30 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:48:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:30 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:48:31 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:48:31 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-03-01 17:48:33 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: "100'" -> Imputer most_frequent # StandardScaler True True # SelectFdr 0.18788055192455086 chi2 # SAM* RandomForestClassifier entropy True False" <class_weight_Trees>  100' False 0.4044792917812593 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:48:33 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-03-01 17:48:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:48:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:48:33 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> RobustScaler False True # SA*  LogisticCV 2 False 450 liblinear None False 0.029935 # RidgeClassifier 1000 False svd 0.050704 3.642024 balanced False False # BernoulliNB 0.272620 8.458216 True # QuadraticDiscriminantAnalysis 0.363860 0.078170 False # RidgeCCV 3 6.847025 None True True # Perceptron l2 8.747352 True 50 False 0.522902 None False # MultinomialNB 7.532114 True # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:48:33 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-03-01 17:49:06 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer median # SelectFromModel 0.421906 False # PolynomialFeatures 5 False False # SA*  MultinomialNB 2.690421 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:49:07 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:49:08 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 500 0.043009 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 True 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:08 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:49:09 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:49:09 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:49:09 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:09 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:10 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:10 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:10 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:49:10 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:49:10 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:10 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:49:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:10 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:49:10 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:10 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:11 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:11 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:11 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:11 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:11 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:12 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:12 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:49:12 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:12 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:12 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 17:49:12 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-03-01 17:49:13 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:85: RuntimeWarning: invalid value encountered in divide
  w1 /= np.sqrt((w1 ** 2).sum())

2018-03-01 17:49:15 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: array must not contain infs or NaNs -> Imputer median # FastICA deflation logcosh 100 0.075698 False 100 # SAM* ExtraTreesClassifier gini False False balanced 100 False 0.9457745734341919 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:49:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:15 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:49:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:16 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:16 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:21 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:21 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:22 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:27 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:27 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 17:49:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:27 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:28 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:49:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:28 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:29 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:49:30 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer mean # StandardScaler True False # SelectPercentile 18 chi2 # PolynomialFeatures 4 True False # SA*  GradientBoostingClassifier exponential 0.054378 0.041077 True 45 False sqrt 65821 0.119384 68681 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:49:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:31 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:31 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:31 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:49:32 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:34 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:34 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:34 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:34 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:35 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:35 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:35 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:49:35 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:35 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:49:35 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 17:49:35 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:49:35 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:49:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:35 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:36 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:49:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:36 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> RobustScaler True True # SelectKBest 3 chi2 # PolynomialFeatures 10 False False # SA*  Perceptron l1 1.968134 True 500 True 0.653789 balanced False # SVC rbf 5 0.019563 1000 balanced # AdaBoostClassifier SAMME 20 1.308442 # Stacking 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:49:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:37 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:39 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:49:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:39 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-03-01 17:49:39 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:39 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:39 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:39 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> RobustScaler False True # SelectFwe 0.733892 f_classif # SA*  ExtraTreeClassifier gini random None False log2 None 0.122478 None # DecisionTreeClassifier entropy best balanced False 0.238361 90063 0.293482 None # GaussianNB # RadiusNeighborsClassifier 7.633566 uniform ball_tree 40 15 minkowski # KNeighborsClassifier 27 uniform auto 98 9 minkowski # SVC linear 8 0.092531 500 balanced # MultinomialNB 2.447395 False # Perceptron l2 5.045849 True 750 True 0.524382 balanced True # LinearDiscriminantAnalysis svd 0.084148 False # Perceptron l1 4.233735 True 25 False 0.553832 None True # LinearDiscriminantAnalysis lsqr 0.030764 False # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:49:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:40 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:49:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:40 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:49:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:40 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:40 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:40 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:41 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:41 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:41 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:49:41 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:41 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:49:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:41 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:49:41 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:49:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:49:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:50:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:12 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:50:12 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:50:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:14 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:50:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:50:15 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:50:15 : /usr/local/lib/python2.7/dist-packages/sklearn/kernel_approximation.py:470: UserWarning: n_components > n_samples. This is not possible.
n_components was set to n_samples, which results in inefficient evaluation of the full kernel.
  warnings.warn("n_components > n_samples. This is not possible.\n"

2018-03-01 17:50:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:50:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-03-01 17:50:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.1 chi2 # SAM* KNeighborsClassifier 2 uniform auto 55 2 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:50:16 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:50:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:50:17 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-03-01 17:50:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer most_frequent # Normalizer max # SelectFwe 0.698184 chi2 # SA*  BernoulliNB 0.143735 7.761086 True # SVC rbf 8 0.052076 500 balanced # Perceptron l1 8.955771 True 50 True 0.015099 balanced False # MultinomialNB 1.995990 True # RidgeClassifier 500 True sparse_cg 0.064756 3.381127 None False False # DecisionTreeClassifier gini best balanced True sqrt None 0.219180 5264 # DecisionTreeClassifier entropy random balanced False None None 0.127408 11507 # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:50:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:50:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:50:17 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '11.628430584359224' -> Imputer most_frequent # SelectPercentile 11.628430584359224 chi2 # SAM* PassiveAggressive  True 5 False hinge False None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:50:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:50:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:50:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:50:20 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:50:20 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:50:20 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:50:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:50:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:20 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:50:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:50:47 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:50:47 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:50:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn("Singular matrix in solving dual problem. Using "

2018-03-01 17:50:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:54 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:50:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:50:59 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:50:59 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:00 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:51:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:00 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 17:51:00 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:51:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:00 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:51:00 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 17:51:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:00 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:51:00 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:51:00 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:51:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:51:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:51:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:51:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:01 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 17:51:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:01 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:51:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:51:01 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:51:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:51:01 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:51:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:51:02 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:51:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:02 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:51:02 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:51:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:51:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:02 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:51:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:03 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:51:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:03 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:51:03 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:51:03 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:51:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:03 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:51:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:03 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:15 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:51:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:51:15 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:51:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:15 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:51:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:51:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:51:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:51:18 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:51:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:51:22 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:51:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:51:41 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:51:41 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:52:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:52:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:52:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:52:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:52:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:52:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:52:57 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:52:58 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:52:58 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:53:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:53:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:53:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 17:54:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:54:00 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:54:00 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:54:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:54:00 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:54:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:54:00 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:54:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:54:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:54:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:54:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:54:01 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:54:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:54:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:54:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:54:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:54:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:54:01 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:54:01 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:54:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:54:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:54:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:54:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:54:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:54:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:54:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:54:17 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:54:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:54:54 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:54:54 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:54:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:00 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 17:55:00 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:55:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:00 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:00 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:00 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:01 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:01 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '58.88123233170863' -> Imputer mean # RobustScaler True True # SelectPercentile 58.88123233170863 chi2 # SAM* GradientBoostingClassifier deviance 0.02102683283349326 0.5778972273820631 auto 480 True 0.2797288369369436 10 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:55:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:12 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:13 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:14 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:55:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:16 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:16 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:17 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:55:17 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:17 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 6.17340242906e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 17:55:17 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 5.92867005324e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 17:55:17 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 3.65078207948e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 17:55:17 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 2.47678389656e-20 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 17:55:17 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 17:55:17 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:55:18 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:55:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:18 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:55:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:18 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 173 clusters where given for a tree with 8 leaves. -> Imputer most_frequent # RobustScaler True True # FeatureAgglomeration cosine complete False 173 #  SAM*' RandomForestClassifier entropy True False balanced_subsample 100 True 0.5565918060287016 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:55:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:19 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:20 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete True 332 # SAM* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 True 220 True 0.6564306719064884 3 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:55:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer mean # StandardScaler True True # FastICA parallel chi2 100 0.090423 False 1 # SAM* GradientBoostingClassifier deviance 0.2155613360930585 0.28870176110739404 True 275 False 0.31988031161984326 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:55:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:23 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:23 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 17:55:23 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:23 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:23 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:55:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:23 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:23 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:55:24 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:24 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:24 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:55:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:24 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:25 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:55:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:25 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:25 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:55:25 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:55:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:55:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:55:26 : /usr/local/lib/python2.7/dist-packages/scipy/optimize/linesearch.py:461: LineSearchWarning: The line search algorithm did not converge
  warn('The line search algorithm did not converge', LineSearchWarning)

2018-03-01 17:55:26 : /usr/local/lib/python2.7/dist-packages/scipy/optimize/linesearch.py:312: LineSearchWarning: The line search algorithm did not converge
  warn('The line search algorithm did not converge', LineSearchWarning)

2018-03-01 17:55:26 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.
  "number of iterations.", ConvergenceWarning)

2018-03-01 17:56:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:56:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:56:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:56:56 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:57:27 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:57:27 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:27 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 17:57:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:27 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:57:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:28 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> Imputer median # StandardScaler True True # SA*  PassiveAggressive  False 100 False epsilon_insensitive True balanced # RidgeCCV 9 8.971695 None True True # LinearDiscriminantAnalysis svd 0.064876 True # NuSVC sigmoid 6 0.087234 100 None # LogisticRegression True 350 liblinear None False 0.031238 # RidgeClassifier 100 True cholesky 0.086712 7.934243 None False True # ExtraTreeClassifier entropy random None auto 0.397730 17852 0.393888 None # RadiusNeighborsClassifier 27.864966 distance auto 74 10 manhattan # SVC rbf 5 0.058470 500 None # PassiveAggressive  True 50 False modified_huber False balanced # Perceptron l1 3.200149 True 25 True 0.417889 balanced True # RidgeCCV None 0.599934 None True False # NuSVC poly 7 0.068272 500 balanced # MultinomialNB 3.258052 True # DecisionTreeClassifier gini best balanced False None 29425 0.047455 60561 # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:57:28 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:57:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:29 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:57:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:29 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:57:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:57:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:29 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:57:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:57:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:30 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:57:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:30 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:57:31 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> FastICA deflation logcosh 1000 0.058259 True 3 # PolynomialFeatures 8 False False # SA*  LinearDiscriminantAnalysis svd 0.066115 True # RidgeClassifier 500 False lsqr 0.036993 0.688571 balanced True False # DecisionTreeClassifier entropy best None auto sqrt None 0.176603 85286 # ExtraTreeClassifier entropy best balanced True 0.867719 None 0.002003 28927 # ExtraTreeClassifier entropy random balanced auto 0.546925 36664 0.200552 None # RidgeClassifier 500 True svd 0.017142 4.126195 balanced True False # GaussianNB # Perceptron l2 1.142194 False 25 True 0.747104 None False # SVC linear 6 0.058804 10 balanced # SVC linear 5 0.084037 10 balanced # KNeighborsClassifier 16 distance ball_tree 61 9 chebyshev # DecisionTreeClassifier entropy random balanced True sqrt 77979 0.266058 None # MultinomialNB 0.964893 True # RadiusNeighborsClassifier 20.018400 uniform brute 95 10 chebyshev # MultinomialNB 2.055547 False # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:57:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:48 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:57:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:57:48 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:48 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:57:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:48 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:57:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:49 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:49 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:57:49 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:49 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:50 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-03-01 17:57:51 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn('FastICA did not converge. Consider increasing '

2018-03-01 17:57:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:52 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:57:52 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:57:52 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:52 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-03-01 17:57:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:53 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:53 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:57:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:53 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:57:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:53 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:57:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:53 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:57:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:57:53 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:57:54 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:57:54 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:57:54 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:57:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:13 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:13 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:58:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:14 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:58:14 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:14 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:14 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:58:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:14 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:58:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:58:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:15 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:15 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:58:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:15 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:58:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:58:16 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:58:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:58:18 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:58:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:19 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:19 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:19 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:58:19 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:58:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:19 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:21 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:58:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:22 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:22 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:58:22 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:58:22 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:25 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:58:25 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:25 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:25 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:26 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:58:26 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:26 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:26 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:58:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:27 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:58:27 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:58:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:27 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:28 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:29 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:58:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:30 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:30 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:30 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:30 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:30 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:58:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:35 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:58:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:52 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:53 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete False 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.001084 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:53 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: n_components=1198 must be between 0 and n_features=8 with svd_solver='full' -> Imputer most_frequent # Normalizer l1 # TraditionalPCA False 1198 # SAM* KNeighborsClassifier 1 uniform auto 53 1 euclidean 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:53 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:58:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:54 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:54 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:54 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:54 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:54 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:55 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:55 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer mean # RobustScaler True False # SelectFdr 0.582525 chi2 # SA*  NuSVC linear 2 0.080776 500 balanced 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 17:58:55 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:55 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:58:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:56 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:56 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:56 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 17:58:56 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:56 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 17:58:56 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:58:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:57 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:58:57 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:57 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:57 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:57 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:57 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:58:57 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:57 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:57 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:58:57 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:58:58 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:58 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:58 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:58:58 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:58:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:00 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 17:59:00 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:59:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:00 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:59:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:00 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:59:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:01 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:02 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:02 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:59:03 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:59:03 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 17:59:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:03 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:03 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:59:03 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:59:03 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:59:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:04 : /usr/local/lib/python2.7/dist-packages/scipy/optimize/linesearch.py:461: LineSearchWarning: The line search algorithm did not converge
  warn('The line search algorithm did not converge', LineSearchWarning)

2018-03-01 17:59:04 : /usr/local/lib/python2.7/dist-packages/scipy/optimize/linesearch.py:312: LineSearchWarning: The line search algorithm did not converge
  warn('The line search algorithm did not converge', LineSearchWarning)

2018-03-01 17:59:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:59:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:59:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:59:19 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:59:20 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:21 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 17:59:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:21 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:59:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:21 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:22 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:59:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 17:59:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:59:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 17:59:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:59:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:59:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 17:59:22 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 17:59:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 17:59:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:22 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:23 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:24 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:04:25 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 18:04:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer mean # StandardScaler True True # SelectPercentile 62 chi2 # SA*  GaussianNB # RidgeClassifier 500 True sparse_cg 0.044064 3.747897 balanced False False # SVC poly 10 0.001818 10 balanced # RidgeClassifier 100 True sparse_cg 0.089380 8.609612 balanced False False # Perceptron l2 8.486397 False 250 True 0.691667 balanced False # NuSVC sigmoid 7 0.083584 100 None # SVC sigmoid 10 0.057568 100 None # Perceptron l2 0.882161 False 10 False 0.871819 balanced True # DecisionTreeClassifier entropy best balanced True sqrt None 0.007176 None # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:04:26 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:26 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 164 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration cosine complete True 164 # SAM* QuadraticDiscriminantAnalysis 0.6396026761675004 0.099188 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:04:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:27 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:28 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:04:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:28 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:04:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:29 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:04:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:31 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:31 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:31 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 4.27142421115e-20 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:04:31 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 6.20028893015e-20 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:04:31 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 4.56264291364e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:04:31 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 5.77183220662e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:04:31 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 4.53176850321e-18 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:04:31 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 5.57100972201e-18 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:04:31 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 5.34448471415e-21 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:04:31 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 6.90720460443e-21 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:04:31 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 1.77696984588e-21 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:04:31 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 1.74945923161e-21 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:04:31 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 2.55374938077e-18 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:04:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:33 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> Imputer median # StandardScaler True True # PolynomialFeatures 7 True True # SA*  MultinomialNB 5.901474 False 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:04:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:35 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:35 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:35 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:35 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:35 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 18:04:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:36 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:04:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:36 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 18:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:04:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.
  "number of iterations.", ConvergenceWarning)

2018-03-01 18:04:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:04:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:39 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:39 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:04:39 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 18:04:39 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:04:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:39 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:04:40 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:04:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:04:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:04:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:04:53 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:05:01 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:05:02 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:05:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:05:03 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:05:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:05:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:05:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:05:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:05:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:05:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:05:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:05:46 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:05:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:05:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:05:46 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:06:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:06:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:06:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:06:31 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:06:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:06:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:06:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:06:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:06:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:07:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:07:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:07:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:07:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:07:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:07:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:08:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:16 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer median # StandardScaler True False # SelectFpr 0.701704 chi2 # SA*  DecisionTreeClassifier entropy random None False None None 0.326174 None # PassiveAggressive  False 500 True perceptron True balanced SA*  LogisticRegression True 450 liblinear None False 0.094066 # NuSVC linear 3 0.047035 500 balanced # SVC sigmoid 2 0.029940 500 None # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:08:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:16 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:08:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:16 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:08:17 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:08:24 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:25 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:25 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> MaxAbsScaler # FastICA deflation logcosh 500 0.051341 True 3 # PolynomialFeatures 4 True False # SA*  GaussianNB # BernoulliNB 0.172918 1.038788 False SA*  DecisionTreeClassifier entropy random balanced False log2 None 0.220375 71106 # MultinomialNB 5.863529 True # Perceptron l2 8.533503 False 100 True 0.015920 balanced False # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:08:26 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:08:26 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:26 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:08:26 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:08:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:26 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 18:08:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:26 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:26 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:26 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:08:26 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:08:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:27 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 18:08:27 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 18:08:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:27 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:08:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:27 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:08:27 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:08:27 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:27 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:27 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:28 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:29 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:08:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:29 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:08:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:08:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:30 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:08:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:30 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:30 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:08:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:30 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:08:30 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:08:30 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:30 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 18:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:32 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:08:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:08:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:08:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:08:32 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:08:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:32 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: invalid literal for int() with base 10: '92.63289395179382' -> Imputer most_frequent # StandardScaler True True # SelectPercentile 92.63289395179382 f_classif # SAM* GradientBoostingClassifier deviance 0.03474109838999682 0.5150113945430513 False 408 True 0.5687034678818491 4 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:08:32 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 18:08:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:08:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:08:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:08:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:08:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 18:09:18 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:09:19 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:09:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:09:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:09:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:09:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:09:58 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:09:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:09:58 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:09:58 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:09:58 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:09:59 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:09:59 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> IncrementalPCA False 2 # PolynomialFeatures 4 True True # SA*  NuSVC rbf 7 0.061963 1000 None # BernoulliNB 0.010494 8.056846 False # RidgeCCV None 4.302190 balanced False False # DecisionTreeClassifier gini random None auto 0.677331 67709 0.439571 79046 # DecisionTreeClassifier gini random None False None 40488 0.211745 9744 # RidgeCCV None 0.485114 balanced True False # MultinomialNB 1.895106 True # QuadraticDiscriminantAnalysis 0.774389 0.036955 False # DecisionTreeClassifier gini best None auto log2 None 0.436060 80916 # PassiveAggressive  True 750 True epsilon_insensitive True balanced # RadiusNeighborsClassifier 1.052400 distance ball_tree 65 9 minkowski # GaussianNB # PassiveAggressive  False 25 False perceptron True balanced # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:09:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:09:59 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:09:59 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:09:59 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:09:59 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:09:59 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:09:59 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 18:09:59 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:10:00 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:10:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:10:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:10:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:10:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 18:10:00 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:10:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:10:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:10:00 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:10:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:10:00 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 18:10:00 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:10:00 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:10:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:10:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:10:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:10:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:04 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:04 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:04 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:04 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:04 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:04 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:04 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:05 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:05 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:05 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:05 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:05 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:05 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:05 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:05 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:06 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:11:06 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:11:06 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:36 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:11:36 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:36 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:11:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:36 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative. -> Imputer most_frequent # RobustScaler False True # SelectPercentile 74 chi2 # SA*  GradientBoostingClassifier exponential 0.096883 0.224630 True 5 True sqrt 13852 0.491621 77229 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:11:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:36 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:37 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:37 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:11:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:37 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:37 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:38 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> Imputer median # TraditionalPCA False 1 # PolynomialFeatures 7 False False # SA*  MultinomialNB 3.704156 False # DecisionTreeClassifier entropy best balanced True None 23802 0.114722 48530 # NuSVC sigmoid 6 0.071774 500 balanced # CentroidClassifier None chebyshev # MultinomialNB 4.342047 False # NuSVC sigmoid 5 0.032623 500 None # BernoulliNB 0.342335 6.600306 False # NuSVC rbf 3 0.021046 500 balanced # ExtraTreeClassifier entropy random balanced False 0.568898 None 0.139995 None # LinearDiscriminantAnalysis svd 0.048936 True # LogisticRegression False 10 newton-cg balanced True 0.022950 # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:11:39 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> RobustScaler False True # SA*  ExtraTreeClassifier gini random None True None None 0.190387 18523 # DecisionTreeClassifier gini best None auto 0.302173 None 0.349201 10465 # QuadraticDiscriminantAnalysis 0.003150 0.070849 True # DecisionTreeClassifier gini best None True 0.279647 70816 0.379844 None # RidgeClassifier 100 True lsqr 0.085728 7.827860 None True True # MultinomialNB 3.029177 False # PassiveAggressive  True 500 True hinge True None # KNeighborsClassifier 1 uniform kd_tree 83 10 euclidean # GaussianNB # PassiveAggressive  True 50 True modified_huber False balanced # ExtraTreeClassifier gini best None auto log2 61917 0.331341 70140 # LinearDiscriminantAnalysis lsqr 0.010064 False # Perceptron l1 6.940291 False 5 False 0.116347 balanced True # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:39 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:42 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:42 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:42 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:11:42 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:42 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:43 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:43 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:11:43 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> StandardScaler True False # SA*  MultinomialNB 3.364969 True 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:11:44 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:44 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:44 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:44 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:44 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:44 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:44 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 18:11:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:47 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 18:11:47 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:47 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:47 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:50 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:52 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:54 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:54 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:54 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:54 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:55 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:56 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Cannot extract more clusters than samples: 332 clusters where given for a tree with 8 leaves. -> Imputer median # FeatureAgglomeration euclidean complete False 332 # SAM* GradientBoostingClassifier deviance 0.24729845478857812 0.8082564085714649 True 220 True 0.6564306719064884 3 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:11:56 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:57 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:57 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-03-01 18:11:57 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:11:59 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.
  "number of iterations.", ConvergenceWarning)

2018-03-01 18:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:13:15 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 18:13:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:13:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:13:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:13:16 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:13:16 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:13:16 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 3.01117538108e-30 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:13:16 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:13:16 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:13:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:13:16 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 18:13:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method median_absolute_error was renamed to neg_median_absolute_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 2.17441638018e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn("Singular matrix in solving dual problem. Using "

2018-03-01 18:13:17 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 4.56699804421e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:13:18 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 4.02764939329e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 18:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:13:18 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:13:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:13:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:13:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:13:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:13:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:13:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:13:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:13:19 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:13:19 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:13:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:13:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:13:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:13:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:13:21 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:13:21 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:13:21 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:13:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:13:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:13:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:13:22 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:13:22 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:13:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:13:22 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:13:22 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:13:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:13:22 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-03-01 18:13:22 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Normalizer l2 # SelectFwe 0.835548 chi2 # SA*  NuSVC sigmoid 2 0.022524 500 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:13:22 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:13:22 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:13:22 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:13:22 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:15:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:15:37 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:37 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:37 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:37 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:15:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:38 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:38 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.
  "number of iterations.", ConvergenceWarning)

2018-03-01 18:15:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:38 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:38 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 18:15:38 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:15:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:45 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 18:15:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:45 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:45 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:15:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:15:47 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:47 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:47 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:47 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:47 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 18:15:47 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Unknown function 'chi2'; should be one of 'logcosh', 'exp', 'cube' or callable -> Imputer most_frequent # RobustScaler True True # FastICA parallel chi2 250 0.061576 False 100 # SAM* ExtraTreesClassifier entropy True False balanced 100 False 0.7062102387181676 None 0.0 None 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:15:48 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:15:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:48 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:15:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:48 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:48 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:49 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> RobustScaler False False # SA*  KNeighborsClassifier 24 distance kd_tree 64 14 minkowski # LogisticCV 4 False 150 newton-cg None False 0.003577 # NuSVC poly 2 0.080687 1000 None # MultinomialNB 5.317044 True # PassiveAggressive  True 100 True log True None # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:49 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:50 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 9.16534371277e-21 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:15:50 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:50 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:50 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> RobustScaler False False # Nystroem poly 7.769900 8 182.491150 3 # SA*  LinearDiscriminantAnalysis lsqr 0.094525 True # MultinomialNB 1.389739 True # SVC poly 9 0.024582 10 balanced # DecisionTreeClassifier gini best balanced auto log2 51194 0.010755 14638 # LogisticCV 7 False 10 sag None False 0.014809 # SVC poly 3 0.048506 10 None # DecisionTreeClassifier entropy best balanced False log2 38898 0.004517 None # MultinomialNB 2.678716 False # NuSVC sigmoid 4 0.080011 100 None # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:15:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:15:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:15:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:15:50 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:15:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:15:50 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:15:50 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:50 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:50 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:15:50 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve
Ill-conditioned matrix detected. Result is not guaranteed to be accurate.
Reciprocal condition number/precision: 2.35088484511e-19 / 1.11022302463e-16
  RuntimeWarning)

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:15:51 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:29 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:30 : /usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.
  UserWarning)

2018-03-01 18:16:30 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Found array with 0 feature(s) (shape=(462, 0)) while a minimum of 1 is required. -> Imputer mean # Normalizer l1 # SelectFpr 0.182865 chi2 # SA*  SVC sigmoid 10 0.056167 500 balanced # NuSVC poly 4 0.015334 10 balanced # BernoulliNB 0.102392 1.895212 True # LogisticRegression True 100 sag balanced True 0.085668 # LinearDiscriminantAnalysis svd 0.037051 True # DecisionTreeClassifier entropy random None False None 45253 0.273985 None # Perceptron l1 8.574071 True 250 False 0.184175 balanced True # RidgeCCV 4 8.264254 None False True # RidgeClassifier 1000 True sag 0.016243 8.386246 None True True # LogisticRegression False 450 sag None True 0.001467 # SVC rbf 9 0.056369 1000 balanced # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:16:30 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:30 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 18:16:30 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:30 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:16:30 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:30 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:30 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:31 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:32 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:33 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:33 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:33 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:33 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/decomposition/fastica_.py:295: UserWarning: Ignoring n_components with whiten=False.
  warnings.warn('Ignoring n_components with whiten=False.')

2018-03-01 18:16:34 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:35 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:16:35 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:35 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:35 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:35 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:35 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:35 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:35 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:36 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:36 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:16:36 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:36 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:36 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:37 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:16:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:37 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:37 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:37 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:37 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:16:37 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:38 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:38 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:38 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:38 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:39 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:16:39 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:39 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:39 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:39 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:39 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.
  "number of iterations.", ConvergenceWarning)

2018-03-01 18:16:39 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:40 : ./recipe/evaluate_algorithm.py:124: UserWarning: ERROR PIPELINE: Input X must be non-negative -> StandardScaler False True # SA*  LogisticRegression True 450 lbfgs balanced True 0.057593 # PassiveAggressive  True 750 False squared_loss False None # MultinomialNB 2.759817 True # PassiveAggressive  True 5 False epsilon_insensitive True balanced # GaussianNB # MultinomialNB 3.654696 True # LogisticRegression False 150 lbfgs balanced False 0.046232 # KNeighborsClassifier 22 uniform ball_tree 88 13 minkowski # BernoulliNB 0.354562 6.285131 False # NuSVC linear 9 0.098229 1000 balanced # PassiveAggressive  False 750 True hinge False None # SVC sigmoid 9 0.032226 100 None # NuSVC poly 2 0.023284 10 balanced # Ensemble 
  warnings.warn("ERROR PIPELINE: "+ str(e) + " -> " + mlAlgorithm,UserWarning)

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:40 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:41 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:41 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:41 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

2018-03-01 18:16:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:41 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.
  warnings.warn(msg, DataConversionWarning)

2018-03-01 18:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2018-03-01 18:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:16:42 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:43 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:44 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:44 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:16:44 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:16:44 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:44 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:44 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:44 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:44 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:16:44 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:44 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:45 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:45 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:45 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:45 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:45 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:16:45 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:16:45 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:16:46 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:16:46 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:16:46 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:17:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:17:13 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:17:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:17:13 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:17:13 : /usr/local/lib/python2.7/dist-packages/sklearn/discriminant_analysis.py:664: DeprecationWarning: 'store_covariances' was renamed to store_covariance in version 0.19 and will be removed in 0.21.
  DeprecationWarning)

2018-03-01 18:17:13 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:17:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:17:13 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:17:13 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:17:13 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:17:13 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:17:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:17:14 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:17:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:17:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:14 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:14 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:17:14 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:17:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:17:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:17:15 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:17:15 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:15 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:17:16 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=10).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.
  DeprecationWarning)

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:17 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:17:18 : /usr/local/lib/python2.7/dist-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.
  warnings.warn("Averaging for metrics other than "

2018-03-01 18:17:18 : /usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

2018-03-01 18:17:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:17:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:17:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:17:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:17:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:17:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:17:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:17:18 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:17:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:497: UserWarning: class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.
  warn('class_weight presets "balanced" or "balanced_subsample" are '

2018-03-01 18:17:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.
  warn("Some inputs do not have OOB scores. "

2018-03-01 18:17:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide
  predictions[k].sum(axis=1)[:, np.newaxis])

2018-03-01 18:17:19 : /usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:305: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.
  warn("Warm-start fitting without increasing n_estimators does not "

2018-03-01 18:17:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

2018-03-01 18:17:19 : /usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)


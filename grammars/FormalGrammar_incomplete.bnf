*<Start> ::= [<preprocessing>] <algorithm>

<preprocessing> ::= <imputation> |  [<inputation>] <binarizer> | [<imputation>] <bounding> | [<imputation>] [<bounding>] <dimensionality definition> 





<imputation> :: mean | median | most_frequent


<bounding> ::= <normalization> | <scaling>
<normalization> ::= l1 | l2

<scaling> ::=   minMax | maxAbs |  (robust | standard [with-std] ) [centering]  		  %robust=scales according to InterQuartile Range
												  %standard = scales to unit variance
 												  %with-scaling =  scale to interquartile range
<dimensionality definition> ::= [<feature selection>] <feature construction> | <feature selection> [<feature construction>]

<feature construction> ::=  <polynomialFeatures>
<polynomialFeatures> ::=  <degree> [interaction-only] [include-bias]            %interaction-only = only interaction features are produced.
<degree> ::= 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 15 | 20 | 25 


<feature selection> ::=  <supervised> | <unsupervised>

<supervised> ::=  <variance>  | <univariate> | (<RFE> | <fromModel>) <estimator>
<variance> ::= threshold

<univariate> ::=  <filter univariate> <score function>
<filter univariate> := selectKBest <k> | selectPercentile <percentile> |  (selectFpr | selectFwe | SelectFdr) <alpha>
<k> :=  {1, numberOfFeatures-1}
<percentile> ::= {0.0, 95.0}
<alpha> ::= {0.0, 1.0}
<score function> ::= f-classif | chi2

<RFE> ::= <featuresToSelect> [<step>] [<cv>]
<featuresToSelect> ::= {1, numberOfFeatures-1}
<step> ::= {0.0, 1.0}							%step = number of features to remove at each iteration
<cv> ::= 3 | 5 | 10

<fromModel> ::= [<threshold from Model>] [<prefit>]			%prefit = Whether a prefit model is expected to be passed or not
<threshold from Model> ::=  float-Value | (median | mean)[*<scalar>]        
<scalar> ::= {0.01, 10.0}						%0.01 and 10.0 are arbitrary numbers
<estimator> ::= <algorithms>


<unsupervised> := <PCA> | <randomProjections>  | <featureAgglomeration>

<PCA> ::= [<components to keep>] [whitening]					%whitening= components are divided by number of samples times 											%%singular values to ensure uncorrelated outputs with unit 											%%%component-wise variances.
<components to keep> ::= {1, numberOfFeatures-1}


<randomProjections> ::= (GaussianRandomProjection | <SparseRandomProjection>) <n components>
<SparseRandomProjection> ::=  <density> [dense_output]          	%density_output=ensure the output of the projection is a dense array 
<density> ::=  {1e-5, 1.0} | 1/sqrt(numberOfFeatures)			%density = Ratio of non-zero component in the random projection matrix
<n components> ::= {1, numberOfFeatures-1}



<featureAgglomeration> ::= <n of clusters> [<connectivity>]  <linkage>  [compute full tree]  
<n of clusters> ::= {1, numberOfFeatures-1}
<connectivity> ::= defineConnctivityMatrix
<linkage> ::= 'ward' (affinity='euclidean')  |  ('complete' | 'average') <affininty>
<affinity> ::= 'euclidean' | 'l1' | 'l2' | 'manhattan' | 'cosine'



****************************************************************************************************************************************************************************************************

<algorithm> ::= <strong models> | <weak models>


<strong models> ::=  <SVM> | <naive Bayes> | <multilayerPerceptron> | <trees>


%ARRUMAR PARAMETROS DO SVM
<SVM> ::= (<SVC> <C>| <NuSVC> <nu>)  <kernel>  <gamma> <coef0> [probability] [shrinking] <tol> [<class_weight>] <max_iter> [<decision_function_shape>]


<kernel> ::= ‘linear’ | ‘poly’ <degree>|‘rbf’ | ‘sigmoid’
<degree> ::= 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 15 | 20 | 25 | 50
<gamma> ::= {2^(-15), 2^(3)}
<coef0> ::= {0.0, 1000.0}				%Independent term in kernel function
<tol> ::= {1e-10, 1e-01}
<class_weight> ::= 'balanced'
<max_iter> ::= -1 | 10 | 100 | 500 | 1000 | 5000 | 7500 | 10000 | 15000 | 30000 | 50000 | 100000
<decision_function_shape> ::= 'ovo' | 'ovr'

<C> ::= {2^(-5), 2^(15)}			%Penalty parameter C of the error term.
<nu> ::= {1e-10, 1.0}				%Upper-bound on the fraction of training errors and 
						%%lower-bound of the fraction of support vectors

<naive Bayes> ::= GaussianNB  | <SpecificNB>
<SpecificNB> ::= (MultinomialNB | BernoulliNB) <alpha> [fit_prior]
<alpha> ::= {0.0, 9.0}


<trees> ::=  (<decisionTree> | <variousEstimators>) [<max_features>] [<max_depth>] [<min_weight_fraction_leaf>] [<max_leaf_nodes>] 
<decisionTree>::= <criterion> <splitter> <class_weight_dts> <presort>
<variousEstimators> :: (<specificTreeEnsemble> | <gradientBoostingClassifier>) <n_estimators> [warm_start]
<specificTreeEnsemble> ::= (<randomForestsClassifier> | <extraTreeClassifier>) <criterion> [bootstrap] [oob_score] [<class_weight_Trees>]
<gradientBoostingClassifier> ::= <loss_gradient> <learning_rate_gradient> [<subsample>] [presort]

<n_estimators> ::= 5 | 10 | 15 | 20 | 25 | 30 | 35 | 40 | 45 | 50
<criterion> ::= 'gini' | 'entropy'
<max_features> ::= {0.01, 1.0} | 'sqrt' | 'log2'
<max_depth>::= {1, numberOfFeatures}
<min_weight_fraction_leaf> ::= {0.0, 0.5}
<max_leaf_nodes> ::= {1, numberOfFeatures}
<class_weight_Trees> ::= 'balanced'| 'balanced_subsample'
<loss_gradient> ::= ‘deviance’ | ‘exponential’
<subsamples> ::= {1e-10, 1.0}
<presort> ::= <boolean> | 'auto'
<learning_rate_gradient> ::= {1e-10, 1.0}
<splitter> ::= 'best' | 'random'


****************************************************************************************************************************************************************************************************
<weak models> ::= <nearest> | <discriminant analysis> | <generalized linear models>
  
<nearest> ::= (<neighbors> | centroid [<shrinking_threshold>]) <metric> 
<neighbors> ::= (kNeighborsClassifier <k> | <radiusNeighborsClassifier>) <weights> <algorithm> <leaf_size>
<RadiusNeighborsClassifier> ::= <radius> <power>

<k> ::= {1, 30}
<weights> ::= 'uniform' | 'distance'
<algorithm> ::= 'auto' | 'brute' | 'kd_tree' | 'ball_tree'
<leaf_size> ::= 10 | 20 | 30 | 50 | 75 | 100
<metric> ::= 'euclidean' |'manhattan' |'chebyshev' |'minkowski'

<radius> ::= {minRangeOfTheFeatures, maxRangeOfTheFeatures}
<shrinking_threshold> ::= {0.0, 30.0}
<p> ::= {1,15}



<discriminant analysis> ::= (<LinearDiscriminantAnalysis> | QuadraticDiscriminantAnalysis <reg_param>) <priors>

<LinearDiscriminantAnalysis> ::= <solver> [<shrinkage>] <n_components> [store_covariance] <tol>		%tol was defined before

<priors> ::= define array[numberOfClasses]
<reg_param>::= {0, 1}
<solver> ::= 'svd' | 'lsqr' | 'eigen'
<shrinkage> ::= {0, 1} | LedoitWolf-Lemma
<n_components> ::= {1, numberOfClasses-1}




%%%%  Various parameters were defined earlier in the other methods of the grammar%%%%

<generalized linear models> ::=  <Logistic> | < | <PassiveAggressive> | <Perceptron> |  <Ridge> | <SGDClassifier>

<Logistic> ::= ( (  (LogisticRegression [warm_start] | <LogisticRegressionCV>) <penalty> [dual] <intercept_scaling>  <class_weight> <max_iter> <solver_lr> <multi_class>)  |  <RandomizedLogisticRegression> ) <C> [fit_intercept] <tol>


<LogisticRegressionCV> ::= <cv> <scoring> [refit] 

<solver_lr> ::= 'newton-cg'| 'lbfgs'| 'liblinear'| 'sag'
<multi_class> ::= 'ovr' | 'multinomial'
<scoring> ::= accuracy_score | average_precision_score | f1_score | precision_score | recall_score | roc_auc_score | fbeta_score
<penalty> ::= 'l1' | 'l2'
<multi_class> ::= 'ovr' | 'crammer_singer'
<intercept_scaling> ::= {-100, 100}


<RandomizedLogisticRegression> ::= <scaling> <sample_fraction> <n_resampling> <selection_threshold> [normalize]
<sample_fraction> ::= {0.0, 1.0}
<n_resampling> ::= 10 | 25 | 50 | 100 | 250 | 500 | 1000
<selection_threshold> ::= {0.1, 0.95}
<scaling> ::= {0.0, 1.0}


<PassiveAggressiveClassifier> ::= <C> [fit_intercept] <n_iter> [shuffle] <loss> [warm_start] <class_weight_pac>
<n_iter> ::= 5 | 10 | 25 | 50 | 100 | 250 | 500 | 750


<Perceptron> ::= <penalty> <alpha> [fit_intercept] <n_iter> [shuffle]  <eta0> <class_weight> [warm_start]
<eta0> ::= {0, 1}

Ridge ::= (<RidgeClassifier> | <RidgeClassifierCV>) <alpha> <class_weight> [normalize] [fit_intercept]
<RidgeClassifier> ::=   <max_iter>  <solver_ridge> <tol>
<RidgeClassifierCV> ::= <alpha> [fit_intercept] <normalize> <scoring> <cv>
<solver_ridge> ::= 'auto'| 'svd'| 'cholesky'| 'lsqr'| 'sparse_cg'| 'sag'


<SGDClassifier> ::= <loss_sgd> <penalty> <alpha> <l1_ratio> [fit_intercept] <n_iter> [shuffle] <epsilon> <learning_rate> <eta0> [<power_t>] <class_weight> [warm_start] <average>
<average> ::= {1, 10}
<loss_sgd> ::= 'hinge'| 'log'| 'modified_huber'| 'squared_hinge'| 'perceptron'| 'squared_loss'| 'huber'| 'epsilon_insensitive'| 'squared_epsilon_insensitive'
<l1_ratio> ::= {0, 1}
<epsilon> ::= {0.0, 1.0}
<power_t> ::= {0.1, 5.0}


****************************************************************************************************************************************************************************************************
<posprocessing> ::=  <chooseMoreAlgorithms> <createEnsemble> | bagging (2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 15 | 20) 
<choose more algorithms> := <algorithm> [<choose more algorithms>]
<createEnsemble> ::=   majority/hardVoting | weighted/softVoting
